{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 1. Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Load the memory-mapped tensor\n",
    "super_tensor_file = os.path.join(current_dir, \"super_emissions_tensor.npy\")\n",
    "molecule_types = [\"CH4\", \"CO2\", \"CO2bio\", \"GWP\", \"N2O\"]\n",
    "data_types = [\"emi\", \"flx\"]\n",
    "\n",
    "# Load the super tensor\n",
    "super_tensor_shape = (5, 2, 288, 1800, 3600)\n",
    "super_tensor = np.memmap(super_tensor_file, dtype='float32', mode='r', shape=super_tensor_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 288, 1800, 3600)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 288, 600, 1200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have already loaded your memory-mapped tensor as:\n",
    "# super_tensor = np.memmap(super_tensor_file, dtype='float32', mode='r', shape=(5, 2, 288, 1800, 3600))\n",
    "\n",
    "# Reshape the last two dimensions so that:\n",
    "# 1800 -> (600, 3) and 3600 -> (1200, 3)\n",
    "reshaped = super_tensor.reshape(5, 2, 288, 600, 3, 1200, 3)\n",
    "\n",
    "# Compute the mean over the 3x3 blocks (axis 4 and axis 6)\n",
    "kernelized_tensor = reshaped.mean(axis=(4, 6))\n",
    "\n",
    "print(kernelized_tensor.shape)  # Expected shape: (5, 2, 288, 600, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"kernelized_tensor.npy\", kernelized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 288, 600, 1200)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 1. Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Load the memory-mapped tensor\n",
    "super_tensor_file = os.path.join(current_dir, \"kernelized_tensor.npy\")\n",
    "molecule_types = [\"CH4\", \"CO2\", \"CO2bio\", \"GWP\", \"N2O\"]\n",
    "data_types = [\"emi\", \"flx\"]\n",
    "\n",
    "# Load the super tensor\n",
    "super_tensor_shape = (5, 2, 288, 600, 1200)\n",
    "kernelized_tensor = np.memmap(super_tensor_file, dtype='float32', mode='r', shape=super_tensor_shape)\n",
    "\n",
    "print(kernelized_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled\n",
      "Training samples: 48\n",
      "Test samples: 12\n",
      "Target (log-transformed, low-res) shape per sample: (75, 150, 5)\n",
      "Y_train_mean (log scale): 0.19016193\n",
      "Y_train_std (log scale): 0.7214341\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56250</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,456,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │           \u001b[38;5;34m768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56250\u001b[0m)          │    \u001b[38;5;34m14,456,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m5\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,588,602</span> (55.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,588,602\u001b[0m (55.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,588,602</span> (55.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,588,602\u001b[0m (55.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss = 1.000012\n",
      "Epoch 002: Loss = 1.000005\n",
      "Epoch 003: Loss = 0.999997\n",
      "Epoch 004: Loss = 0.999989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:17:28.252377: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005: Loss = 0.999981\n",
      "Epoch 006: Loss = 0.999972\n",
      "Epoch 007: Loss = 0.999964\n",
      "Epoch 008: Loss = 0.999955\n",
      "Epoch 009: Loss = 0.999947\n",
      "Epoch 010: Loss = 0.999938\n",
      "Epoch 011: Loss = 0.999929\n",
      "Epoch 012: Loss = 0.999921\n",
      "Epoch 013: Loss = 0.999912\n",
      "Epoch 014: Loss = 0.999903\n",
      "Epoch 015: Loss = 0.999894\n",
      "Epoch 016: Loss = 0.999885\n",
      "Epoch 017: Loss = 0.999876\n",
      "Epoch 018: Loss = 0.999867\n",
      "Epoch 019: Loss = 0.999859\n",
      "Epoch 020: Loss = 0.999849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:17:33.975019: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021: Loss = 0.999840\n",
      "Epoch 022: Loss = 0.999831\n",
      "Epoch 023: Loss = 0.999822\n",
      "Epoch 024: Loss = 0.999813\n",
      "Epoch 025: Loss = 0.999804\n",
      "Epoch 026: Loss = 0.999794\n",
      "Epoch 027: Loss = 0.999785\n",
      "Epoch 028: Loss = 0.999776\n",
      "Epoch 029: Loss = 0.999766\n",
      "Epoch 030: Loss = 0.999757\n",
      "Epoch 031: Loss = 0.999747\n",
      "Epoch 032: Loss = 0.999738\n",
      "Epoch 033: Loss = 0.999728\n",
      "Epoch 034: Loss = 0.999719\n",
      "Epoch 035: Loss = 0.999709\n",
      "Epoch 036: Loss = 0.999700\n",
      "Epoch 037: Loss = 0.999690\n",
      "Epoch 038: Loss = 0.999680\n",
      "Epoch 039: Loss = 0.999670\n",
      "Epoch 040: Loss = 0.999660\n",
      "Epoch 041: Loss = 0.999650\n",
      "Epoch 042: Loss = 0.999640\n",
      "Epoch 043: Loss = 0.999631\n",
      "Epoch 044: Loss = 0.999620\n",
      "Epoch 045: Loss = 0.999611\n",
      "Epoch 046: Loss = 0.999601\n",
      "Epoch 047: Loss = 0.999591\n",
      "Epoch 048: Loss = 0.999581\n",
      "Epoch 049: Loss = 0.999570\n",
      "Epoch 050: Loss = 0.999560\n",
      "Epoch 051: Loss = 0.999550\n",
      "Epoch 052: Loss = 0.999539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:17:45.602818: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 053: Loss = 0.999529\n",
      "Epoch 054: Loss = 0.999519\n",
      "Epoch 055: Loss = 0.999509\n",
      "Epoch 056: Loss = 0.999498\n",
      "Epoch 057: Loss = 0.999488\n",
      "Epoch 058: Loss = 0.999478\n",
      "Epoch 059: Loss = 0.999467\n",
      "Epoch 060: Loss = 0.999456\n",
      "Epoch 061: Loss = 0.999446\n",
      "Epoch 062: Loss = 0.999435\n",
      "Epoch 063: Loss = 0.999425\n",
      "Epoch 064: Loss = 0.999414\n",
      "Epoch 065: Loss = 0.999403\n",
      "Epoch 066: Loss = 0.999392\n",
      "Epoch 067: Loss = 0.999381\n",
      "Epoch 068: Loss = 0.999371\n",
      "Epoch 069: Loss = 0.999360\n",
      "Epoch 070: Loss = 0.999349\n",
      "Epoch 071: Loss = 0.999338\n",
      "Epoch 072: Loss = 0.999327\n",
      "Epoch 073: Loss = 0.999317\n",
      "Epoch 074: Loss = 0.999306\n",
      "Epoch 075: Loss = 0.999295\n",
      "Epoch 076: Loss = 0.999284\n",
      "Epoch 077: Loss = 0.999273\n",
      "Epoch 078: Loss = 0.999262\n",
      "Epoch 079: Loss = 0.999251\n",
      "Epoch 080: Loss = 0.999239\n",
      "Epoch 081: Loss = 0.999228\n",
      "Epoch 082: Loss = 0.999217\n",
      "Epoch 083: Loss = 0.999206\n",
      "Epoch 084: Loss = 0.999195\n",
      "Epoch 085: Loss = 0.999184\n",
      "Epoch 086: Loss = 0.999172\n",
      "Epoch 087: Loss = 0.999161\n",
      "Epoch 088: Loss = 0.999150\n",
      "Epoch 089: Loss = 0.999138\n",
      "Epoch 090: Loss = 0.999127\n",
      "Epoch 091: Loss = 0.999115\n",
      "Epoch 092: Loss = 0.999104\n",
      "Epoch 093: Loss = 0.999093\n",
      "Epoch 094: Loss = 0.999082\n",
      "Epoch 095: Loss = 0.999070\n",
      "Epoch 096: Loss = 0.999059\n",
      "Epoch 097: Loss = 0.999047\n",
      "Epoch 098: Loss = 0.999035\n",
      "Epoch 099: Loss = 0.999024\n",
      "Epoch 100: Loss = 0.999012\n",
      "Epoch 101: Loss = 0.999001\n",
      "Epoch 102: Loss = 0.998989\n",
      "Epoch 103: Loss = 0.998977\n",
      "Epoch 104: Loss = 0.998966\n",
      "Epoch 105: Loss = 0.998954\n",
      "Epoch 106: Loss = 0.998942\n",
      "Epoch 107: Loss = 0.998930\n",
      "Epoch 108: Loss = 0.998919\n",
      "Epoch 109: Loss = 0.998907\n",
      "Epoch 110: Loss = 0.998895\n",
      "Epoch 111: Loss = 0.998883\n",
      "Epoch 112: Loss = 0.998872\n",
      "Epoch 113: Loss = 0.998860\n",
      "Epoch 114: Loss = 0.998848\n",
      "Epoch 115: Loss = 0.998836\n",
      "Epoch 116: Loss = 0.998824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:18:08.608252: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117: Loss = 0.998812\n",
      "Epoch 118: Loss = 0.998800\n",
      "Epoch 119: Loss = 0.998788\n",
      "Epoch 120: Loss = 0.998776\n",
      "Epoch 121: Loss = 0.998764\n",
      "Epoch 122: Loss = 0.998752\n",
      "Epoch 123: Loss = 0.998740\n",
      "Epoch 124: Loss = 0.998728\n",
      "Epoch 125: Loss = 0.998716\n",
      "Epoch 126: Loss = 0.998703\n",
      "Epoch 127: Loss = 0.998691\n",
      "Epoch 128: Loss = 0.998679\n",
      "Epoch 129: Loss = 0.998667\n",
      "Epoch 130: Loss = 0.998654\n",
      "Epoch 131: Loss = 0.998642\n",
      "Epoch 132: Loss = 0.998630\n",
      "Epoch 133: Loss = 0.998618\n",
      "Epoch 134: Loss = 0.998605\n",
      "Epoch 135: Loss = 0.998593\n",
      "Epoch 136: Loss = 0.998581\n",
      "Epoch 137: Loss = 0.998568\n",
      "Epoch 138: Loss = 0.998556\n",
      "Epoch 139: Loss = 0.998544\n",
      "Epoch 140: Loss = 0.998531\n",
      "Epoch 141: Loss = 0.998519\n",
      "Epoch 142: Loss = 0.998506\n",
      "Epoch 143: Loss = 0.998493\n",
      "Epoch 144: Loss = 0.998481\n",
      "Epoch 145: Loss = 0.998469\n",
      "Epoch 146: Loss = 0.998456\n",
      "Epoch 147: Loss = 0.998443\n",
      "Epoch 148: Loss = 0.998431\n",
      "Epoch 149: Loss = 0.998418\n",
      "Epoch 150: Loss = 0.998406\n",
      "Epoch 151: Loss = 0.998393\n",
      "Epoch 152: Loss = 0.998380\n",
      "Epoch 153: Loss = 0.998367\n",
      "Epoch 154: Loss = 0.998355\n",
      "Epoch 155: Loss = 0.998342\n",
      "Epoch 156: Loss = 0.998329\n",
      "Epoch 157: Loss = 0.998316\n",
      "Epoch 158: Loss = 0.998304\n",
      "Epoch 159: Loss = 0.998290\n",
      "Epoch 160: Loss = 0.998278\n",
      "Epoch 161: Loss = 0.998265\n",
      "Epoch 162: Loss = 0.998252\n",
      "Epoch 163: Loss = 0.998239\n",
      "Epoch 164: Loss = 0.998226\n",
      "Epoch 165: Loss = 0.998213\n",
      "Epoch 166: Loss = 0.998200\n",
      "Epoch 167: Loss = 0.998187\n",
      "Epoch 168: Loss = 0.998174\n",
      "Epoch 169: Loss = 0.998161\n",
      "Epoch 170: Loss = 0.998148\n",
      "Epoch 171: Loss = 0.998135\n",
      "Epoch 172: Loss = 0.998122\n",
      "Epoch 173: Loss = 0.998109\n",
      "Epoch 174: Loss = 0.998096\n",
      "Epoch 175: Loss = 0.998082\n",
      "Epoch 176: Loss = 0.998069\n",
      "Epoch 177: Loss = 0.998056\n",
      "Epoch 178: Loss = 0.998043\n",
      "Epoch 179: Loss = 0.998029\n",
      "Epoch 180: Loss = 0.998016\n",
      "Epoch 181: Loss = 0.998003\n",
      "Epoch 182: Loss = 0.997989\n",
      "Epoch 183: Loss = 0.997976\n",
      "Epoch 184: Loss = 0.997962\n",
      "Epoch 185: Loss = 0.997949\n",
      "Epoch 186: Loss = 0.997936\n",
      "Epoch 187: Loss = 0.997922\n",
      "Epoch 188: Loss = 0.997909\n",
      "Epoch 189: Loss = 0.997895\n",
      "Epoch 190: Loss = 0.997882\n",
      "Epoch 191: Loss = 0.997868\n",
      "Epoch 192: Loss = 0.997855\n",
      "Epoch 193: Loss = 0.997841\n",
      "Epoch 194: Loss = 0.997828\n",
      "Epoch 195: Loss = 0.997814\n",
      "Epoch 196: Loss = 0.997800\n",
      "Epoch 197: Loss = 0.997787\n",
      "Epoch 198: Loss = 0.997773\n",
      "Epoch 199: Loss = 0.997759\n",
      "Epoch 200: Loss = 0.997745\n",
      "Epoch 201: Loss = 0.997731\n",
      "Epoch 202: Loss = 0.997718\n",
      "Epoch 203: Loss = 0.997704\n",
      "Epoch 204: Loss = 0.997690\n",
      "Epoch 205: Loss = 0.997676\n",
      "Epoch 206: Loss = 0.997662\n",
      "Epoch 207: Loss = 0.997648\n",
      "Epoch 208: Loss = 0.997635\n",
      "Epoch 209: Loss = 0.997621\n",
      "Epoch 210: Loss = 0.997606\n",
      "Epoch 211: Loss = 0.997592\n",
      "Epoch 212: Loss = 0.997578\n",
      "Epoch 213: Loss = 0.997565\n",
      "Epoch 214: Loss = 0.997551\n",
      "Epoch 215: Loss = 0.997536\n",
      "Epoch 216: Loss = 0.997522\n",
      "Epoch 217: Loss = 0.997508\n",
      "Epoch 218: Loss = 0.997494\n",
      "Epoch 219: Loss = 0.997480\n",
      "Epoch 220: Loss = 0.997466\n",
      "Epoch 221: Loss = 0.997451\n",
      "Epoch 222: Loss = 0.997437\n",
      "Epoch 223: Loss = 0.997423\n",
      "Epoch 224: Loss = 0.997408\n",
      "Epoch 225: Loss = 0.997394\n",
      "Epoch 226: Loss = 0.997379\n",
      "Epoch 227: Loss = 0.997365\n",
      "Epoch 228: Loss = 0.997351\n",
      "Epoch 229: Loss = 0.997336\n",
      "Epoch 230: Loss = 0.997322\n",
      "Epoch 231: Loss = 0.997307\n",
      "Epoch 232: Loss = 0.997293\n",
      "Epoch 233: Loss = 0.997278\n",
      "Epoch 234: Loss = 0.997264\n",
      "Epoch 235: Loss = 0.997250\n",
      "Epoch 236: Loss = 0.997235\n",
      "Epoch 237: Loss = 0.997220\n",
      "Epoch 238: Loss = 0.997205\n",
      "Epoch 239: Loss = 0.997191\n",
      "Epoch 240: Loss = 0.997176\n",
      "Epoch 241: Loss = 0.997162\n",
      "Epoch 242: Loss = 0.997147\n",
      "Epoch 243: Loss = 0.997132\n",
      "Epoch 244: Loss = 0.997117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:18:54.597978: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245: Loss = 0.997102\n",
      "Epoch 246: Loss = 0.997088\n",
      "Epoch 247: Loss = 0.997072\n",
      "Epoch 248: Loss = 0.997058\n",
      "Epoch 249: Loss = 0.997043\n",
      "Epoch 250: Loss = 0.997028\n",
      "Epoch 251: Loss = 0.997013\n",
      "Epoch 252: Loss = 0.996998\n",
      "Epoch 253: Loss = 0.996983\n",
      "Epoch 254: Loss = 0.996968\n",
      "Epoch 255: Loss = 0.996953\n",
      "Epoch 256: Loss = 0.996938\n",
      "Epoch 257: Loss = 0.996922\n",
      "Epoch 258: Loss = 0.996907\n",
      "Epoch 259: Loss = 0.996892\n",
      "Epoch 260: Loss = 0.996877\n",
      "Epoch 261: Loss = 0.996862\n",
      "Epoch 262: Loss = 0.996846\n",
      "Epoch 263: Loss = 0.996831\n",
      "Epoch 264: Loss = 0.996816\n",
      "Epoch 265: Loss = 0.996800\n",
      "Epoch 266: Loss = 0.996785\n",
      "Epoch 267: Loss = 0.996770\n",
      "Epoch 268: Loss = 0.996754\n",
      "Epoch 269: Loss = 0.996739\n",
      "Epoch 270: Loss = 0.996723\n",
      "Epoch 271: Loss = 0.996707\n",
      "Epoch 272: Loss = 0.996692\n",
      "Epoch 273: Loss = 0.996676\n",
      "Epoch 274: Loss = 0.996661\n",
      "Epoch 275: Loss = 0.996645\n",
      "Epoch 276: Loss = 0.996629\n",
      "Epoch 277: Loss = 0.996614\n",
      "Epoch 278: Loss = 0.996598\n",
      "Epoch 279: Loss = 0.996582\n",
      "Epoch 280: Loss = 0.996567\n",
      "Epoch 281: Loss = 0.996551\n",
      "Epoch 282: Loss = 0.996535\n",
      "Epoch 283: Loss = 0.996519\n",
      "Epoch 284: Loss = 0.996503\n",
      "Epoch 285: Loss = 0.996487\n",
      "Epoch 286: Loss = 0.996471\n",
      "Epoch 287: Loss = 0.996455\n",
      "Epoch 288: Loss = 0.996439\n",
      "Epoch 289: Loss = 0.996423\n",
      "Epoch 290: Loss = 0.996407\n",
      "Epoch 291: Loss = 0.996391\n",
      "Epoch 292: Loss = 0.996375\n",
      "Epoch 293: Loss = 0.996358\n",
      "Epoch 294: Loss = 0.996342\n",
      "Epoch 295: Loss = 0.996326\n",
      "Epoch 296: Loss = 0.996309\n",
      "Epoch 297: Loss = 0.996293\n",
      "Epoch 298: Loss = 0.996277\n",
      "Epoch 299: Loss = 0.996260\n",
      "Epoch 300: Loss = 0.996244\n",
      "Epoch 301: Loss = 0.996227\n",
      "Epoch 302: Loss = 0.996211\n",
      "Epoch 303: Loss = 0.996195\n",
      "Epoch 304: Loss = 0.996178\n",
      "Epoch 305: Loss = 0.996161\n",
      "Epoch 306: Loss = 0.996145\n",
      "Epoch 307: Loss = 0.996128\n",
      "Epoch 308: Loss = 0.996112\n",
      "Epoch 309: Loss = 0.996095\n",
      "Epoch 310: Loss = 0.996078\n",
      "Epoch 311: Loss = 0.996061\n",
      "Epoch 312: Loss = 0.996044\n",
      "Epoch 313: Loss = 0.996027\n",
      "Epoch 314: Loss = 0.996011\n",
      "Epoch 315: Loss = 0.995994\n",
      "Epoch 316: Loss = 0.995977\n",
      "Epoch 317: Loss = 0.995960\n",
      "Epoch 318: Loss = 0.995943\n",
      "Epoch 319: Loss = 0.995925\n",
      "Epoch 320: Loss = 0.995908\n",
      "Epoch 321: Loss = 0.995891\n",
      "Epoch 322: Loss = 0.995874\n",
      "Epoch 323: Loss = 0.995857\n",
      "Epoch 324: Loss = 0.995840\n",
      "Epoch 325: Loss = 0.995822\n",
      "Epoch 326: Loss = 0.995805\n",
      "Epoch 327: Loss = 0.995788\n",
      "Epoch 328: Loss = 0.995771\n",
      "Epoch 329: Loss = 0.995753\n",
      "Epoch 330: Loss = 0.995736\n",
      "Epoch 331: Loss = 0.995718\n",
      "Epoch 332: Loss = 0.995701\n",
      "Epoch 333: Loss = 0.995684\n",
      "Epoch 334: Loss = 0.995666\n",
      "Epoch 335: Loss = 0.995648\n",
      "Epoch 336: Loss = 0.995631\n",
      "Epoch 337: Loss = 0.995613\n",
      "Epoch 338: Loss = 0.995595\n",
      "Epoch 339: Loss = 0.995578\n",
      "Epoch 340: Loss = 0.995560\n",
      "Epoch 341: Loss = 0.995542\n",
      "Epoch 342: Loss = 0.995524\n",
      "Epoch 343: Loss = 0.995506\n",
      "Epoch 344: Loss = 0.995488\n",
      "Epoch 345: Loss = 0.995470\n",
      "Epoch 346: Loss = 0.995452\n",
      "Epoch 347: Loss = 0.995434\n",
      "Epoch 348: Loss = 0.995416\n",
      "Epoch 349: Loss = 0.995398\n",
      "Epoch 350: Loss = 0.995380\n",
      "Epoch 351: Loss = 0.995362\n",
      "Epoch 352: Loss = 0.995344\n",
      "Epoch 353: Loss = 0.995326\n",
      "Epoch 354: Loss = 0.995308\n",
      "Epoch 355: Loss = 0.995289\n",
      "Epoch 356: Loss = 0.995271\n",
      "Epoch 357: Loss = 0.995253\n",
      "Epoch 358: Loss = 0.995234\n",
      "Epoch 359: Loss = 0.995216\n",
      "Epoch 360: Loss = 0.995197\n",
      "Epoch 361: Loss = 0.995179\n",
      "Epoch 362: Loss = 0.995160\n",
      "Epoch 363: Loss = 0.995142\n",
      "Epoch 364: Loss = 0.995123\n",
      "Epoch 365: Loss = 0.995105\n",
      "Epoch 366: Loss = 0.995085\n",
      "Epoch 367: Loss = 0.995067\n",
      "Epoch 368: Loss = 0.995048\n",
      "Epoch 369: Loss = 0.995029\n",
      "Epoch 370: Loss = 0.995010\n",
      "Epoch 371: Loss = 0.994991\n",
      "Epoch 372: Loss = 0.994973\n",
      "Epoch 373: Loss = 0.994953\n",
      "Epoch 374: Loss = 0.994935\n",
      "Epoch 375: Loss = 0.994916\n",
      "Epoch 376: Loss = 0.994897\n",
      "Epoch 377: Loss = 0.994878\n",
      "Epoch 378: Loss = 0.994859\n",
      "Epoch 379: Loss = 0.994840\n",
      "Epoch 380: Loss = 0.994820\n",
      "Epoch 381: Loss = 0.994802\n",
      "Epoch 382: Loss = 0.994782\n",
      "Epoch 383: Loss = 0.994763\n",
      "Epoch 384: Loss = 0.994743\n",
      "Epoch 385: Loss = 0.994724\n",
      "Epoch 386: Loss = 0.994704\n",
      "Epoch 387: Loss = 0.994685\n",
      "Epoch 388: Loss = 0.994666\n",
      "Epoch 389: Loss = 0.994646\n",
      "Epoch 390: Loss = 0.994626\n",
      "Epoch 391: Loss = 0.994607\n",
      "Epoch 392: Loss = 0.994587\n",
      "Epoch 393: Loss = 0.994567\n",
      "Epoch 394: Loss = 0.994547\n",
      "Epoch 395: Loss = 0.994527\n",
      "Epoch 396: Loss = 0.994508\n",
      "Epoch 397: Loss = 0.994487\n",
      "Epoch 398: Loss = 0.994468\n",
      "Epoch 399: Loss = 0.994448\n",
      "Epoch 400: Loss = 0.994428\n",
      "Epoch 401: Loss = 0.994408\n",
      "Epoch 402: Loss = 0.994389\n",
      "Epoch 403: Loss = 0.994368\n",
      "Epoch 404: Loss = 0.994348\n",
      "Epoch 405: Loss = 0.994328\n",
      "Epoch 406: Loss = 0.994308\n",
      "Epoch 407: Loss = 0.994288\n",
      "Epoch 408: Loss = 0.994267\n",
      "Epoch 409: Loss = 0.994247\n",
      "Epoch 410: Loss = 0.994226\n",
      "Epoch 411: Loss = 0.994206\n",
      "Epoch 412: Loss = 0.994186\n",
      "Epoch 413: Loss = 0.994165\n",
      "Epoch 414: Loss = 0.994144\n",
      "Epoch 415: Loss = 0.994124\n",
      "Epoch 416: Loss = 0.994102\n",
      "Epoch 417: Loss = 0.994082\n",
      "Epoch 418: Loss = 0.994062\n",
      "Epoch 419: Loss = 0.994042\n",
      "Epoch 420: Loss = 0.994020\n",
      "Epoch 421: Loss = 0.994000\n",
      "Epoch 422: Loss = 0.993979\n",
      "Epoch 423: Loss = 0.993958\n",
      "Epoch 424: Loss = 0.993937\n",
      "Epoch 425: Loss = 0.993916\n",
      "Epoch 426: Loss = 0.993895\n",
      "Epoch 427: Loss = 0.993874\n",
      "Epoch 428: Loss = 0.993853\n",
      "Epoch 429: Loss = 0.993832\n",
      "Epoch 430: Loss = 0.993811\n",
      "Epoch 431: Loss = 0.993790\n",
      "Epoch 432: Loss = 0.993769\n",
      "Epoch 433: Loss = 0.993747\n",
      "Epoch 434: Loss = 0.993726\n",
      "Epoch 435: Loss = 0.993704\n",
      "Epoch 436: Loss = 0.993683\n",
      "Epoch 437: Loss = 0.993661\n",
      "Epoch 438: Loss = 0.993640\n",
      "Epoch 439: Loss = 0.993618\n",
      "Epoch 440: Loss = 0.993596\n",
      "Epoch 441: Loss = 0.993574\n",
      "Epoch 442: Loss = 0.993553\n",
      "Epoch 443: Loss = 0.993531\n",
      "Epoch 444: Loss = 0.993509\n",
      "Epoch 445: Loss = 0.993488\n",
      "Epoch 446: Loss = 0.993466\n",
      "Epoch 447: Loss = 0.993444\n",
      "Epoch 448: Loss = 0.993422\n",
      "Epoch 449: Loss = 0.993400\n",
      "Epoch 450: Loss = 0.993378\n",
      "Epoch 451: Loss = 0.993357\n",
      "Epoch 452: Loss = 0.993334\n",
      "Epoch 453: Loss = 0.993312\n",
      "Epoch 454: Loss = 0.993290\n",
      "Epoch 455: Loss = 0.993268\n",
      "Epoch 456: Loss = 0.993246\n",
      "Epoch 457: Loss = 0.993224\n",
      "Epoch 458: Loss = 0.993201\n",
      "Epoch 459: Loss = 0.993179\n",
      "Epoch 460: Loss = 0.993157\n",
      "Epoch 461: Loss = 0.993134\n",
      "Epoch 462: Loss = 0.993112\n",
      "Epoch 463: Loss = 0.993089\n",
      "Epoch 464: Loss = 0.993067\n",
      "Epoch 465: Loss = 0.993044\n",
      "Epoch 466: Loss = 0.993021\n",
      "Epoch 467: Loss = 0.992998\n",
      "Epoch 468: Loss = 0.992975\n",
      "Epoch 469: Loss = 0.992953\n",
      "Epoch 470: Loss = 0.992930\n",
      "Epoch 471: Loss = 0.992907\n",
      "Epoch 472: Loss = 0.992884\n",
      "Epoch 473: Loss = 0.992861\n",
      "Epoch 474: Loss = 0.992838\n",
      "Epoch 475: Loss = 0.992815\n",
      "Epoch 476: Loss = 0.992793\n",
      "Epoch 477: Loss = 0.992770\n",
      "Epoch 478: Loss = 0.992746\n",
      "Epoch 479: Loss = 0.992724\n",
      "Epoch 480: Loss = 0.992701\n",
      "Epoch 481: Loss = 0.992677\n",
      "Epoch 482: Loss = 0.992654\n",
      "Epoch 483: Loss = 0.992630\n",
      "Epoch 484: Loss = 0.992606\n",
      "Epoch 485: Loss = 0.992583\n",
      "Epoch 486: Loss = 0.992560\n",
      "Epoch 487: Loss = 0.992536\n",
      "Epoch 488: Loss = 0.992512\n",
      "Epoch 489: Loss = 0.992488\n",
      "Epoch 490: Loss = 0.992464\n",
      "Epoch 491: Loss = 0.992441\n",
      "Epoch 492: Loss = 0.992417\n",
      "Epoch 493: Loss = 0.992393\n",
      "Epoch 494: Loss = 0.992369\n",
      "Epoch 495: Loss = 0.992345\n",
      "Epoch 496: Loss = 0.992321\n",
      "Epoch 497: Loss = 0.992298\n",
      "Epoch 498: Loss = 0.992274\n",
      "Epoch 499: Loss = 0.992250\n",
      "Epoch 500: Loss = 0.992226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:20:27.086768: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501: Loss = 0.992203\n",
      "Epoch 502: Loss = 0.992179\n",
      "Epoch 503: Loss = 0.992154\n",
      "Epoch 504: Loss = 0.992131\n",
      "Epoch 505: Loss = 0.992106\n",
      "Epoch 506: Loss = 0.992082\n",
      "Epoch 507: Loss = 0.992057\n",
      "Epoch 508: Loss = 0.992032\n",
      "Epoch 509: Loss = 0.992008\n",
      "Epoch 510: Loss = 0.991984\n",
      "Epoch 511: Loss = 0.991959\n",
      "Epoch 512: Loss = 0.991934\n",
      "Epoch 513: Loss = 0.991909\n",
      "Epoch 514: Loss = 0.991884\n",
      "Epoch 515: Loss = 0.991859\n",
      "Epoch 516: Loss = 0.991835\n",
      "Epoch 517: Loss = 0.991810\n",
      "Epoch 518: Loss = 0.991785\n",
      "Epoch 519: Loss = 0.991760\n",
      "Epoch 520: Loss = 0.991735\n",
      "Epoch 521: Loss = 0.991710\n",
      "Epoch 522: Loss = 0.991685\n",
      "Epoch 523: Loss = 0.991660\n",
      "Epoch 524: Loss = 0.991635\n",
      "Epoch 525: Loss = 0.991610\n",
      "Epoch 526: Loss = 0.991585\n",
      "Epoch 527: Loss = 0.991560\n",
      "Epoch 528: Loss = 0.991534\n",
      "Epoch 529: Loss = 0.991509\n",
      "Epoch 530: Loss = 0.991484\n",
      "Epoch 531: Loss = 0.991458\n",
      "Epoch 532: Loss = 0.991433\n",
      "Epoch 533: Loss = 0.991407\n",
      "Epoch 534: Loss = 0.991381\n",
      "Epoch 535: Loss = 0.991356\n",
      "Epoch 536: Loss = 0.991329\n",
      "Epoch 537: Loss = 0.991304\n",
      "Epoch 538: Loss = 0.991278\n",
      "Epoch 539: Loss = 0.991252\n",
      "Epoch 540: Loss = 0.991226\n",
      "Epoch 541: Loss = 0.991201\n",
      "Epoch 542: Loss = 0.991174\n",
      "Epoch 543: Loss = 0.991149\n",
      "Epoch 544: Loss = 0.991123\n",
      "Epoch 545: Loss = 0.991097\n",
      "Epoch 546: Loss = 0.991071\n",
      "Epoch 547: Loss = 0.991045\n",
      "Epoch 548: Loss = 0.991018\n",
      "Epoch 549: Loss = 0.990992\n",
      "Epoch 550: Loss = 0.990966\n",
      "Epoch 551: Loss = 0.990940\n",
      "Epoch 552: Loss = 0.990914\n",
      "Epoch 553: Loss = 0.990887\n",
      "Epoch 554: Loss = 0.990861\n",
      "Epoch 555: Loss = 0.990834\n",
      "Epoch 556: Loss = 0.990808\n",
      "Epoch 557: Loss = 0.990781\n",
      "Epoch 558: Loss = 0.990754\n",
      "Epoch 559: Loss = 0.990727\n",
      "Epoch 560: Loss = 0.990700\n",
      "Epoch 561: Loss = 0.990673\n",
      "Epoch 562: Loss = 0.990647\n",
      "Epoch 563: Loss = 0.990620\n",
      "Epoch 564: Loss = 0.990593\n",
      "Epoch 565: Loss = 0.990565\n",
      "Epoch 566: Loss = 0.990539\n",
      "Epoch 567: Loss = 0.990512\n",
      "Epoch 568: Loss = 0.990484\n",
      "Epoch 569: Loss = 0.990458\n",
      "Epoch 570: Loss = 0.990430\n",
      "Epoch 571: Loss = 0.990403\n",
      "Epoch 572: Loss = 0.990376\n",
      "Epoch 573: Loss = 0.990348\n",
      "Epoch 574: Loss = 0.990321\n",
      "Epoch 575: Loss = 0.990294\n",
      "Epoch 576: Loss = 0.990267\n",
      "Epoch 577: Loss = 0.990239\n",
      "Epoch 578: Loss = 0.990211\n",
      "Epoch 579: Loss = 0.990184\n",
      "Epoch 580: Loss = 0.990156\n",
      "Epoch 581: Loss = 0.990129\n",
      "Epoch 582: Loss = 0.990100\n",
      "Epoch 583: Loss = 0.990073\n",
      "Epoch 584: Loss = 0.990045\n",
      "Epoch 585: Loss = 0.990016\n",
      "Epoch 586: Loss = 0.989988\n",
      "Epoch 587: Loss = 0.989960\n",
      "Epoch 588: Loss = 0.989932\n",
      "Epoch 589: Loss = 0.989905\n",
      "Epoch 590: Loss = 0.989876\n",
      "Epoch 591: Loss = 0.989848\n",
      "Epoch 592: Loss = 0.989819\n",
      "Epoch 593: Loss = 0.989791\n",
      "Epoch 594: Loss = 0.989763\n",
      "Epoch 595: Loss = 0.989735\n",
      "Epoch 596: Loss = 0.989706\n",
      "Epoch 597: Loss = 0.989677\n",
      "Epoch 598: Loss = 0.989649\n",
      "Epoch 599: Loss = 0.989620\n",
      "Epoch 600: Loss = 0.989592\n",
      "Epoch 601: Loss = 0.989564\n",
      "Epoch 602: Loss = 0.989535\n",
      "Epoch 603: Loss = 0.989506\n",
      "Epoch 604: Loss = 0.989478\n",
      "Epoch 605: Loss = 0.989449\n",
      "Epoch 606: Loss = 0.989420\n",
      "Epoch 607: Loss = 0.989390\n",
      "Epoch 608: Loss = 0.989361\n",
      "Epoch 609: Loss = 0.989332\n",
      "Epoch 610: Loss = 0.989302\n",
      "Epoch 611: Loss = 0.989272\n",
      "Epoch 612: Loss = 0.989243\n",
      "Epoch 613: Loss = 0.989214\n",
      "Epoch 614: Loss = 0.989185\n",
      "Epoch 615: Loss = 0.989156\n",
      "Epoch 616: Loss = 0.989127\n",
      "Epoch 617: Loss = 0.989097\n",
      "Epoch 618: Loss = 0.989068\n",
      "Epoch 619: Loss = 0.989038\n",
      "Epoch 620: Loss = 0.989008\n",
      "Epoch 621: Loss = 0.988979\n",
      "Epoch 622: Loss = 0.988949\n",
      "Epoch 623: Loss = 0.988919\n",
      "Epoch 624: Loss = 0.988889\n",
      "Epoch 625: Loss = 0.988860\n",
      "Epoch 626: Loss = 0.988831\n",
      "Epoch 627: Loss = 0.988801\n",
      "Epoch 628: Loss = 0.988771\n",
      "Epoch 629: Loss = 0.988741\n",
      "Epoch 630: Loss = 0.988711\n",
      "Epoch 631: Loss = 0.988680\n",
      "Epoch 632: Loss = 0.988650\n",
      "Epoch 633: Loss = 0.988620\n",
      "Epoch 634: Loss = 0.988590\n",
      "Epoch 635: Loss = 0.988558\n",
      "Epoch 636: Loss = 0.988527\n",
      "Epoch 637: Loss = 0.988497\n",
      "Epoch 638: Loss = 0.988467\n",
      "Epoch 639: Loss = 0.988436\n",
      "Epoch 640: Loss = 0.988406\n",
      "Epoch 641: Loss = 0.988376\n",
      "Epoch 642: Loss = 0.988344\n",
      "Epoch 643: Loss = 0.988314\n",
      "Epoch 644: Loss = 0.988283\n",
      "Epoch 645: Loss = 0.988252\n",
      "Epoch 646: Loss = 0.988222\n",
      "Epoch 647: Loss = 0.988191\n",
      "Epoch 648: Loss = 0.988159\n",
      "Epoch 649: Loss = 0.988128\n",
      "Epoch 650: Loss = 0.988097\n",
      "Epoch 651: Loss = 0.988067\n",
      "Epoch 652: Loss = 0.988036\n",
      "Epoch 653: Loss = 0.988005\n",
      "Epoch 654: Loss = 0.987974\n",
      "Epoch 655: Loss = 0.987942\n",
      "Epoch 656: Loss = 0.987910\n",
      "Epoch 657: Loss = 0.987879\n",
      "Epoch 658: Loss = 0.987848\n",
      "Epoch 659: Loss = 0.987816\n",
      "Epoch 660: Loss = 0.987784\n",
      "Epoch 661: Loss = 0.987751\n",
      "Epoch 662: Loss = 0.987720\n",
      "Epoch 663: Loss = 0.987688\n",
      "Epoch 664: Loss = 0.987657\n",
      "Epoch 665: Loss = 0.987625\n",
      "Epoch 666: Loss = 0.987593\n",
      "Epoch 667: Loss = 0.987561\n",
      "Epoch 668: Loss = 0.987529\n",
      "Epoch 669: Loss = 0.987498\n",
      "Epoch 670: Loss = 0.987465\n",
      "Epoch 671: Loss = 0.987434\n",
      "Epoch 672: Loss = 0.987401\n",
      "Epoch 673: Loss = 0.987369\n",
      "Epoch 674: Loss = 0.987336\n",
      "Epoch 675: Loss = 0.987305\n",
      "Epoch 676: Loss = 0.987273\n",
      "Epoch 677: Loss = 0.987241\n",
      "Epoch 678: Loss = 0.987209\n",
      "Epoch 679: Loss = 0.987176\n",
      "Epoch 680: Loss = 0.987143\n",
      "Epoch 681: Loss = 0.987109\n",
      "Epoch 682: Loss = 0.987076\n",
      "Epoch 683: Loss = 0.987043\n",
      "Epoch 684: Loss = 0.987010\n",
      "Epoch 685: Loss = 0.986977\n",
      "Epoch 686: Loss = 0.986943\n",
      "Epoch 687: Loss = 0.986911\n",
      "Epoch 688: Loss = 0.986878\n",
      "Epoch 689: Loss = 0.986846\n",
      "Epoch 690: Loss = 0.986813\n",
      "Epoch 691: Loss = 0.986779\n",
      "Epoch 692: Loss = 0.986746\n",
      "Epoch 693: Loss = 0.986712\n",
      "Epoch 694: Loss = 0.986679\n",
      "Epoch 695: Loss = 0.986645\n",
      "Epoch 696: Loss = 0.986613\n",
      "Epoch 697: Loss = 0.986578\n",
      "Epoch 698: Loss = 0.986545\n",
      "Epoch 699: Loss = 0.986511\n",
      "Epoch 700: Loss = 0.986478\n",
      "Epoch 701: Loss = 0.986444\n",
      "Epoch 702: Loss = 0.986412\n",
      "Epoch 703: Loss = 0.986378\n",
      "Epoch 704: Loss = 0.986344\n",
      "Epoch 705: Loss = 0.986310\n",
      "Epoch 706: Loss = 0.986275\n",
      "Epoch 707: Loss = 0.986241\n",
      "Epoch 708: Loss = 0.986207\n",
      "Epoch 709: Loss = 0.986173\n",
      "Epoch 710: Loss = 0.986138\n",
      "Epoch 711: Loss = 0.986104\n",
      "Epoch 712: Loss = 0.986069\n",
      "Epoch 713: Loss = 0.986035\n",
      "Epoch 714: Loss = 0.986001\n",
      "Epoch 715: Loss = 0.985967\n",
      "Epoch 716: Loss = 0.985933\n",
      "Epoch 717: Loss = 0.985898\n",
      "Epoch 718: Loss = 0.985863\n",
      "Epoch 719: Loss = 0.985828\n",
      "Epoch 720: Loss = 0.985794\n",
      "Epoch 721: Loss = 0.985760\n",
      "Epoch 722: Loss = 0.985724\n",
      "Epoch 723: Loss = 0.985689\n",
      "Epoch 724: Loss = 0.985654\n",
      "Epoch 725: Loss = 0.985619\n",
      "Epoch 726: Loss = 0.985585\n",
      "Epoch 727: Loss = 0.985550\n",
      "Epoch 728: Loss = 0.985516\n",
      "Epoch 729: Loss = 0.985480\n",
      "Epoch 730: Loss = 0.985445\n",
      "Epoch 731: Loss = 0.985409\n",
      "Epoch 732: Loss = 0.985373\n",
      "Epoch 733: Loss = 0.985337\n",
      "Epoch 734: Loss = 0.985301\n",
      "Epoch 735: Loss = 0.985265\n",
      "Epoch 736: Loss = 0.985229\n",
      "Epoch 737: Loss = 0.985193\n",
      "Epoch 738: Loss = 0.985158\n",
      "Epoch 739: Loss = 0.985123\n",
      "Epoch 740: Loss = 0.985088\n",
      "Epoch 741: Loss = 0.985052\n",
      "Epoch 742: Loss = 0.985015\n",
      "Epoch 743: Loss = 0.984979\n",
      "Epoch 744: Loss = 0.984943\n",
      "Epoch 745: Loss = 0.984907\n",
      "Epoch 746: Loss = 0.984871\n",
      "Epoch 747: Loss = 0.984834\n",
      "Epoch 748: Loss = 0.984799\n",
      "Epoch 749: Loss = 0.984762\n",
      "Epoch 750: Loss = 0.984726\n",
      "Epoch 751: Loss = 0.984690\n",
      "Epoch 752: Loss = 0.984654\n",
      "Epoch 753: Loss = 0.984619\n",
      "Epoch 754: Loss = 0.984582\n",
      "Epoch 755: Loss = 0.984545\n",
      "Epoch 756: Loss = 0.984507\n",
      "Epoch 757: Loss = 0.984471\n",
      "Epoch 758: Loss = 0.984433\n",
      "Epoch 759: Loss = 0.984395\n",
      "Epoch 760: Loss = 0.984358\n",
      "Epoch 761: Loss = 0.984321\n",
      "Epoch 762: Loss = 0.984283\n",
      "Epoch 763: Loss = 0.984246\n",
      "Epoch 764: Loss = 0.984210\n",
      "Epoch 765: Loss = 0.984173\n",
      "Epoch 766: Loss = 0.984136\n",
      "Epoch 767: Loss = 0.984098\n",
      "Epoch 768: Loss = 0.984061\n",
      "Epoch 769: Loss = 0.984024\n",
      "Epoch 770: Loss = 0.983987\n",
      "Epoch 771: Loss = 0.983949\n",
      "Epoch 772: Loss = 0.983911\n",
      "Epoch 773: Loss = 0.983873\n",
      "Epoch 774: Loss = 0.983836\n",
      "Epoch 775: Loss = 0.983797\n",
      "Epoch 776: Loss = 0.983760\n",
      "Epoch 777: Loss = 0.983722\n",
      "Epoch 778: Loss = 0.983685\n",
      "Epoch 779: Loss = 0.983647\n",
      "Epoch 780: Loss = 0.983609\n",
      "Epoch 781: Loss = 0.983570\n",
      "Epoch 782: Loss = 0.983532\n",
      "Epoch 783: Loss = 0.983492\n",
      "Epoch 784: Loss = 0.983453\n",
      "Epoch 785: Loss = 0.983415\n",
      "Epoch 786: Loss = 0.983377\n",
      "Epoch 787: Loss = 0.983338\n",
      "Epoch 788: Loss = 0.983299\n",
      "Epoch 789: Loss = 0.983260\n",
      "Epoch 790: Loss = 0.983222\n",
      "Epoch 791: Loss = 0.983183\n",
      "Epoch 792: Loss = 0.983145\n",
      "Epoch 793: Loss = 0.983107\n",
      "Epoch 794: Loss = 0.983068\n",
      "Epoch 795: Loss = 0.983029\n",
      "Epoch 796: Loss = 0.982990\n",
      "Epoch 797: Loss = 0.982950\n",
      "Epoch 798: Loss = 0.982912\n",
      "Epoch 799: Loss = 0.982873\n",
      "Epoch 800: Loss = 0.982834\n",
      "Epoch 801: Loss = 0.982795\n",
      "Epoch 802: Loss = 0.982755\n",
      "Epoch 803: Loss = 0.982716\n",
      "Epoch 804: Loss = 0.982676\n",
      "Epoch 805: Loss = 0.982637\n",
      "Epoch 806: Loss = 0.982597\n",
      "Epoch 807: Loss = 0.982557\n",
      "Epoch 808: Loss = 0.982516\n",
      "Epoch 809: Loss = 0.982475\n",
      "Epoch 810: Loss = 0.982434\n",
      "Epoch 811: Loss = 0.982395\n",
      "Epoch 812: Loss = 0.982355\n",
      "Epoch 813: Loss = 0.982316\n",
      "Epoch 814: Loss = 0.982275\n",
      "Epoch 815: Loss = 0.982235\n",
      "Epoch 816: Loss = 0.982195\n",
      "Epoch 817: Loss = 0.982154\n",
      "Epoch 818: Loss = 0.982114\n",
      "Epoch 819: Loss = 0.982074\n",
      "Epoch 820: Loss = 0.982034\n",
      "Epoch 821: Loss = 0.981993\n",
      "Epoch 822: Loss = 0.981952\n",
      "Epoch 823: Loss = 0.981912\n",
      "Epoch 824: Loss = 0.981871\n",
      "Epoch 825: Loss = 0.981830\n",
      "Epoch 826: Loss = 0.981790\n",
      "Epoch 827: Loss = 0.981749\n",
      "Epoch 828: Loss = 0.981709\n",
      "Epoch 829: Loss = 0.981667\n",
      "Epoch 830: Loss = 0.981626\n",
      "Epoch 831: Loss = 0.981585\n",
      "Epoch 832: Loss = 0.981544\n",
      "Epoch 833: Loss = 0.981502\n",
      "Epoch 834: Loss = 0.981460\n",
      "Epoch 835: Loss = 0.981418\n",
      "Epoch 836: Loss = 0.981377\n",
      "Epoch 837: Loss = 0.981336\n",
      "Epoch 838: Loss = 0.981294\n",
      "Epoch 839: Loss = 0.981253\n",
      "Epoch 840: Loss = 0.981211\n",
      "Epoch 841: Loss = 0.981170\n",
      "Epoch 842: Loss = 0.981128\n",
      "Epoch 843: Loss = 0.981086\n",
      "Epoch 844: Loss = 0.981044\n",
      "Epoch 845: Loss = 0.981002\n",
      "Epoch 846: Loss = 0.980960\n",
      "Epoch 847: Loss = 0.980918\n",
      "Epoch 848: Loss = 0.980876\n",
      "Epoch 849: Loss = 0.980834\n",
      "Epoch 850: Loss = 0.980793\n",
      "Epoch 851: Loss = 0.980751\n",
      "Epoch 852: Loss = 0.980708\n",
      "Epoch 853: Loss = 0.980666\n",
      "Epoch 854: Loss = 0.980624\n",
      "Epoch 855: Loss = 0.980581\n",
      "Epoch 856: Loss = 0.980538\n",
      "Epoch 857: Loss = 0.980495\n",
      "Epoch 858: Loss = 0.980451\n",
      "Epoch 859: Loss = 0.980407\n",
      "Epoch 860: Loss = 0.980364\n",
      "Epoch 861: Loss = 0.980321\n",
      "Epoch 862: Loss = 0.980279\n",
      "Epoch 863: Loss = 0.980236\n",
      "Epoch 864: Loss = 0.980193\n",
      "Epoch 865: Loss = 0.980149\n",
      "Epoch 866: Loss = 0.980105\n",
      "Epoch 867: Loss = 0.980062\n",
      "Epoch 868: Loss = 0.980019\n",
      "Epoch 869: Loss = 0.979975\n",
      "Epoch 870: Loss = 0.979933\n",
      "Epoch 871: Loss = 0.979889\n",
      "Epoch 872: Loss = 0.979846\n",
      "Epoch 873: Loss = 0.979802\n",
      "Epoch 874: Loss = 0.979758\n",
      "Epoch 875: Loss = 0.979715\n",
      "Epoch 876: Loss = 0.979671\n",
      "Epoch 877: Loss = 0.979628\n",
      "Epoch 878: Loss = 0.979584\n",
      "Epoch 879: Loss = 0.979540\n",
      "Epoch 880: Loss = 0.979496\n",
      "Epoch 881: Loss = 0.979451\n",
      "Epoch 882: Loss = 0.979406\n",
      "Epoch 883: Loss = 0.979361\n",
      "Epoch 884: Loss = 0.979315\n",
      "Epoch 885: Loss = 0.979271\n",
      "Epoch 886: Loss = 0.979227\n",
      "Epoch 887: Loss = 0.979182\n",
      "Epoch 888: Loss = 0.979138\n",
      "Epoch 889: Loss = 0.979093\n",
      "Epoch 890: Loss = 0.979048\n",
      "Epoch 891: Loss = 0.979003\n",
      "Epoch 892: Loss = 0.978958\n",
      "Epoch 893: Loss = 0.978913\n",
      "Epoch 894: Loss = 0.978867\n",
      "Epoch 895: Loss = 0.978823\n",
      "Epoch 896: Loss = 0.978778\n",
      "Epoch 897: Loss = 0.978733\n",
      "Epoch 898: Loss = 0.978689\n",
      "Epoch 899: Loss = 0.978643\n",
      "Epoch 900: Loss = 0.978598\n",
      "Epoch 901: Loss = 0.978553\n",
      "Epoch 902: Loss = 0.978507\n",
      "Epoch 903: Loss = 0.978462\n",
      "Epoch 904: Loss = 0.978416\n",
      "Epoch 905: Loss = 0.978371\n",
      "Epoch 906: Loss = 0.978324\n",
      "Epoch 907: Loss = 0.978278\n",
      "Epoch 908: Loss = 0.978231\n",
      "Epoch 909: Loss = 0.978184\n",
      "Epoch 910: Loss = 0.978139\n",
      "Epoch 911: Loss = 0.978092\n",
      "Epoch 912: Loss = 0.978046\n",
      "Epoch 913: Loss = 0.978000\n",
      "Epoch 914: Loss = 0.977953\n",
      "Epoch 915: Loss = 0.977908\n",
      "Epoch 916: Loss = 0.977861\n",
      "Epoch 917: Loss = 0.977815\n",
      "Epoch 918: Loss = 0.977768\n",
      "Epoch 919: Loss = 0.977721\n",
      "Epoch 920: Loss = 0.977675\n",
      "Epoch 921: Loss = 0.977629\n",
      "Epoch 922: Loss = 0.977582\n",
      "Epoch 923: Loss = 0.977536\n",
      "Epoch 924: Loss = 0.977490\n",
      "Epoch 925: Loss = 0.977442\n",
      "Epoch 926: Loss = 0.977395\n",
      "Epoch 927: Loss = 0.977348\n",
      "Epoch 928: Loss = 0.977302\n",
      "Epoch 929: Loss = 0.977254\n",
      "Epoch 930: Loss = 0.977206\n",
      "Epoch 931: Loss = 0.977158\n",
      "Epoch 932: Loss = 0.977110\n",
      "Epoch 933: Loss = 0.977063\n",
      "Epoch 934: Loss = 0.977014\n",
      "Epoch 935: Loss = 0.976966\n",
      "Epoch 936: Loss = 0.976919\n",
      "Epoch 937: Loss = 0.976872\n",
      "Epoch 938: Loss = 0.976824\n",
      "Epoch 939: Loss = 0.976776\n",
      "Epoch 940: Loss = 0.976728\n",
      "Epoch 941: Loss = 0.976679\n",
      "Epoch 942: Loss = 0.976631\n",
      "Epoch 943: Loss = 0.976582\n",
      "Epoch 944: Loss = 0.976533\n",
      "Epoch 945: Loss = 0.976486\n",
      "Epoch 946: Loss = 0.976439\n",
      "Epoch 947: Loss = 0.976390\n",
      "Epoch 948: Loss = 0.976343\n",
      "Epoch 949: Loss = 0.976294\n",
      "Epoch 950: Loss = 0.976245\n",
      "Epoch 951: Loss = 0.976197\n",
      "Epoch 952: Loss = 0.976149\n",
      "Epoch 953: Loss = 0.976101\n",
      "Epoch 954: Loss = 0.976051\n",
      "Epoch 955: Loss = 0.976002\n",
      "Epoch 956: Loss = 0.975952\n",
      "Epoch 957: Loss = 0.975902\n",
      "Epoch 958: Loss = 0.975853\n",
      "Epoch 959: Loss = 0.975804\n",
      "Epoch 960: Loss = 0.975755\n",
      "Epoch 961: Loss = 0.975705\n",
      "Epoch 962: Loss = 0.975656\n",
      "Epoch 963: Loss = 0.975607\n",
      "Epoch 964: Loss = 0.975557\n",
      "Epoch 965: Loss = 0.975508\n",
      "Epoch 966: Loss = 0.975458\n",
      "Epoch 967: Loss = 0.975408\n",
      "Epoch 968: Loss = 0.975357\n",
      "Epoch 969: Loss = 0.975307\n",
      "Epoch 970: Loss = 0.975257\n",
      "Epoch 971: Loss = 0.975208\n",
      "Epoch 972: Loss = 0.975158\n",
      "Epoch 973: Loss = 0.975108\n",
      "Epoch 974: Loss = 0.975058\n",
      "Epoch 975: Loss = 0.975008\n",
      "Epoch 976: Loss = 0.974958\n",
      "Epoch 977: Loss = 0.974908\n",
      "Epoch 978: Loss = 0.974858\n",
      "Epoch 979: Loss = 0.974807\n",
      "Epoch 980: Loss = 0.974756\n",
      "Epoch 981: Loss = 0.974704\n",
      "Epoch 982: Loss = 0.974653\n",
      "Epoch 983: Loss = 0.974600\n",
      "Epoch 984: Loss = 0.974550\n",
      "Epoch 985: Loss = 0.974499\n",
      "Epoch 986: Loss = 0.974449\n",
      "Epoch 987: Loss = 0.974398\n",
      "Epoch 988: Loss = 0.974347\n",
      "Epoch 989: Loss = 0.974296\n",
      "Epoch 990: Loss = 0.974244\n",
      "Epoch 991: Loss = 0.974192\n",
      "Epoch 992: Loss = 0.974140\n",
      "Epoch 993: Loss = 0.974088\n",
      "Epoch 994: Loss = 0.974036\n",
      "Epoch 995: Loss = 0.973985\n",
      "Epoch 996: Loss = 0.973934\n",
      "Epoch 997: Loss = 0.973883\n",
      "Epoch 998: Loss = 0.973831\n",
      "Epoch 999: Loss = 0.973780\n",
      "Epoch 1000: Loss = 0.973728\n",
      "Epoch 1001: Loss = 0.973677\n",
      "Epoch 1002: Loss = 0.973626\n",
      "Epoch 1003: Loss = 0.973574\n",
      "Epoch 1004: Loss = 0.973522\n",
      "Epoch 1005: Loss = 0.973469\n",
      "Epoch 1006: Loss = 0.973416\n",
      "Epoch 1007: Loss = 0.973362\n",
      "Epoch 1008: Loss = 0.973308\n",
      "Epoch 1009: Loss = 0.973255\n",
      "Epoch 1010: Loss = 0.973203\n",
      "Epoch 1011: Loss = 0.973151\n",
      "Epoch 1012: Loss = 0.973099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:23:32.037817: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1013: Loss = 0.973046\n",
      "Epoch 1014: Loss = 0.972993\n",
      "Epoch 1015: Loss = 0.972940\n",
      "Epoch 1016: Loss = 0.972887\n",
      "Epoch 1017: Loss = 0.972833\n",
      "Epoch 1018: Loss = 0.972779\n",
      "Epoch 1019: Loss = 0.972725\n",
      "Epoch 1020: Loss = 0.972672\n",
      "Epoch 1021: Loss = 0.972618\n",
      "Epoch 1022: Loss = 0.972566\n",
      "Epoch 1023: Loss = 0.972513\n",
      "Epoch 1024: Loss = 0.972459\n",
      "Epoch 1025: Loss = 0.972407\n",
      "Epoch 1026: Loss = 0.972353\n",
      "Epoch 1027: Loss = 0.972299\n",
      "Epoch 1028: Loss = 0.972244\n",
      "Epoch 1029: Loss = 0.972190\n",
      "Epoch 1030: Loss = 0.972136\n",
      "Epoch 1031: Loss = 0.972081\n",
      "Epoch 1032: Loss = 0.972026\n",
      "Epoch 1033: Loss = 0.971970\n",
      "Epoch 1034: Loss = 0.971916\n",
      "Epoch 1035: Loss = 0.971862\n",
      "Epoch 1036: Loss = 0.971808\n",
      "Epoch 1037: Loss = 0.971753\n",
      "Epoch 1038: Loss = 0.971699\n",
      "Epoch 1039: Loss = 0.971645\n",
      "Epoch 1040: Loss = 0.971590\n",
      "Epoch 1041: Loss = 0.971535\n",
      "Epoch 1042: Loss = 0.971480\n",
      "Epoch 1043: Loss = 0.971425\n",
      "Epoch 1044: Loss = 0.971370\n",
      "Epoch 1045: Loss = 0.971316\n",
      "Epoch 1046: Loss = 0.971261\n",
      "Epoch 1047: Loss = 0.971206\n",
      "Epoch 1048: Loss = 0.971151\n",
      "Epoch 1049: Loss = 0.971096\n",
      "Epoch 1050: Loss = 0.971041\n",
      "Epoch 1051: Loss = 0.970986\n",
      "Epoch 1052: Loss = 0.970931\n",
      "Epoch 1053: Loss = 0.970875\n",
      "Epoch 1054: Loss = 0.970819\n",
      "Epoch 1055: Loss = 0.970762\n",
      "Epoch 1056: Loss = 0.970705\n",
      "Epoch 1057: Loss = 0.970649\n",
      "Epoch 1058: Loss = 0.970591\n",
      "Epoch 1059: Loss = 0.970535\n",
      "Epoch 1060: Loss = 0.970479\n",
      "Epoch 1061: Loss = 0.970423\n",
      "Epoch 1062: Loss = 0.970368\n",
      "Epoch 1063: Loss = 0.970312\n",
      "Epoch 1064: Loss = 0.970256\n",
      "Epoch 1065: Loss = 0.970199\n",
      "Epoch 1066: Loss = 0.970142\n",
      "Epoch 1067: Loss = 0.970084\n",
      "Epoch 1068: Loss = 0.970027\n",
      "Epoch 1069: Loss = 0.969970\n",
      "Epoch 1070: Loss = 0.969914\n",
      "Epoch 1071: Loss = 0.969858\n",
      "Epoch 1072: Loss = 0.969801\n",
      "Epoch 1073: Loss = 0.969744\n",
      "Epoch 1074: Loss = 0.969687\n",
      "Epoch 1075: Loss = 0.969631\n",
      "Epoch 1076: Loss = 0.969575\n",
      "Epoch 1077: Loss = 0.969518\n",
      "Epoch 1078: Loss = 0.969459\n",
      "Epoch 1079: Loss = 0.969402\n",
      "Epoch 1080: Loss = 0.969344\n",
      "Epoch 1081: Loss = 0.969284\n",
      "Epoch 1082: Loss = 0.969225\n",
      "Epoch 1083: Loss = 0.969166\n",
      "Epoch 1084: Loss = 0.969108\n",
      "Epoch 1085: Loss = 0.969052\n",
      "Epoch 1086: Loss = 0.968994\n",
      "Epoch 1087: Loss = 0.968937\n",
      "Epoch 1088: Loss = 0.968879\n",
      "Epoch 1089: Loss = 0.968821\n",
      "Epoch 1090: Loss = 0.968763\n",
      "Epoch 1091: Loss = 0.968704\n",
      "Epoch 1092: Loss = 0.968645\n",
      "Epoch 1093: Loss = 0.968585\n",
      "Epoch 1094: Loss = 0.968527\n",
      "Epoch 1095: Loss = 0.968468\n",
      "Epoch 1096: Loss = 0.968409\n",
      "Epoch 1097: Loss = 0.968352\n",
      "Epoch 1098: Loss = 0.968293\n",
      "Epoch 1099: Loss = 0.968235\n",
      "Epoch 1100: Loss = 0.968177\n",
      "Epoch 1101: Loss = 0.968118\n",
      "Epoch 1102: Loss = 0.968060\n",
      "Epoch 1103: Loss = 0.968001\n",
      "Epoch 1104: Loss = 0.967942\n",
      "Epoch 1105: Loss = 0.967882\n",
      "Epoch 1106: Loss = 0.967820\n",
      "Epoch 1107: Loss = 0.967760\n",
      "Epoch 1108: Loss = 0.967698\n",
      "Epoch 1109: Loss = 0.967638\n",
      "Epoch 1110: Loss = 0.967577\n",
      "Epoch 1111: Loss = 0.967518\n",
      "Epoch 1112: Loss = 0.967459\n",
      "Epoch 1113: Loss = 0.967400\n",
      "Epoch 1114: Loss = 0.967341\n",
      "Epoch 1115: Loss = 0.967280\n",
      "Epoch 1116: Loss = 0.967218\n",
      "Epoch 1117: Loss = 0.967157\n",
      "Epoch 1118: Loss = 0.967096\n",
      "Epoch 1119: Loss = 0.967036\n",
      "Epoch 1120: Loss = 0.966976\n",
      "Epoch 1121: Loss = 0.966915\n",
      "Epoch 1122: Loss = 0.966855\n",
      "Epoch 1123: Loss = 0.966794\n",
      "Epoch 1124: Loss = 0.966733\n",
      "Epoch 1125: Loss = 0.966674\n",
      "Epoch 1126: Loss = 0.966614\n",
      "Epoch 1127: Loss = 0.966555\n",
      "Epoch 1128: Loss = 0.966494\n",
      "Epoch 1129: Loss = 0.966433\n",
      "Epoch 1130: Loss = 0.966371\n",
      "Epoch 1131: Loss = 0.966308\n",
      "Epoch 1132: Loss = 0.966245\n",
      "Epoch 1133: Loss = 0.966183\n",
      "Epoch 1134: Loss = 0.966120\n",
      "Epoch 1135: Loss = 0.966058\n",
      "Epoch 1136: Loss = 0.965996\n",
      "Epoch 1137: Loss = 0.965935\n",
      "Epoch 1138: Loss = 0.965874\n",
      "Epoch 1139: Loss = 0.965812\n",
      "Epoch 1140: Loss = 0.965750\n",
      "Epoch 1141: Loss = 0.965687\n",
      "Epoch 1142: Loss = 0.965624\n",
      "Epoch 1143: Loss = 0.965561\n",
      "Epoch 1144: Loss = 0.965499\n",
      "Epoch 1145: Loss = 0.965437\n",
      "Epoch 1146: Loss = 0.965375\n",
      "Epoch 1147: Loss = 0.965313\n",
      "Epoch 1148: Loss = 0.965250\n",
      "Epoch 1149: Loss = 0.965187\n",
      "Epoch 1150: Loss = 0.965126\n",
      "Epoch 1151: Loss = 0.965064\n",
      "Epoch 1152: Loss = 0.965001\n",
      "Epoch 1153: Loss = 0.964939\n",
      "Epoch 1154: Loss = 0.964876\n",
      "Epoch 1155: Loss = 0.964813\n",
      "Epoch 1156: Loss = 0.964747\n",
      "Epoch 1157: Loss = 0.964682\n",
      "Epoch 1158: Loss = 0.964619\n",
      "Epoch 1159: Loss = 0.964555\n",
      "Epoch 1160: Loss = 0.964491\n",
      "Epoch 1161: Loss = 0.964428\n",
      "Epoch 1162: Loss = 0.964365\n",
      "Epoch 1163: Loss = 0.964301\n",
      "Epoch 1164: Loss = 0.964238\n",
      "Epoch 1165: Loss = 0.964174\n",
      "Epoch 1166: Loss = 0.964108\n",
      "Epoch 1167: Loss = 0.964043\n",
      "Epoch 1168: Loss = 0.963979\n",
      "Epoch 1169: Loss = 0.963914\n",
      "Epoch 1170: Loss = 0.963850\n",
      "Epoch 1171: Loss = 0.963788\n",
      "Epoch 1172: Loss = 0.963724\n",
      "Epoch 1173: Loss = 0.963659\n",
      "Epoch 1174: Loss = 0.963593\n",
      "Epoch 1175: Loss = 0.963530\n",
      "Epoch 1176: Loss = 0.963466\n",
      "Epoch 1177: Loss = 0.963402\n",
      "Epoch 1178: Loss = 0.963338\n",
      "Epoch 1179: Loss = 0.963275\n",
      "Epoch 1180: Loss = 0.963208\n",
      "Epoch 1181: Loss = 0.963142\n",
      "Epoch 1182: Loss = 0.963074\n",
      "Epoch 1183: Loss = 0.963008\n",
      "Epoch 1184: Loss = 0.962942\n",
      "Epoch 1185: Loss = 0.962877\n",
      "Epoch 1186: Loss = 0.962812\n",
      "Epoch 1187: Loss = 0.962746\n",
      "Epoch 1188: Loss = 0.962681\n",
      "Epoch 1189: Loss = 0.962616\n",
      "Epoch 1190: Loss = 0.962550\n",
      "Epoch 1191: Loss = 0.962483\n",
      "Epoch 1192: Loss = 0.962416\n",
      "Epoch 1193: Loss = 0.962349\n",
      "Epoch 1194: Loss = 0.962283\n",
      "Epoch 1195: Loss = 0.962216\n",
      "Epoch 1196: Loss = 0.962152\n",
      "Epoch 1197: Loss = 0.962086\n",
      "Epoch 1198: Loss = 0.962019\n",
      "Epoch 1199: Loss = 0.961953\n",
      "Epoch 1200: Loss = 0.961887\n",
      "Epoch 1201: Loss = 0.961821\n",
      "Epoch 1202: Loss = 0.961754\n",
      "Epoch 1203: Loss = 0.961688\n",
      "Epoch 1204: Loss = 0.961622\n",
      "Epoch 1205: Loss = 0.961556\n",
      "Epoch 1206: Loss = 0.961487\n",
      "Epoch 1207: Loss = 0.961418\n",
      "Epoch 1208: Loss = 0.961349\n",
      "Epoch 1209: Loss = 0.961282\n",
      "Epoch 1210: Loss = 0.961216\n",
      "Epoch 1211: Loss = 0.961149\n",
      "Epoch 1212: Loss = 0.961081\n",
      "Epoch 1213: Loss = 0.961014\n",
      "Epoch 1214: Loss = 0.960946\n",
      "Epoch 1215: Loss = 0.960878\n",
      "Epoch 1216: Loss = 0.960810\n",
      "Epoch 1217: Loss = 0.960740\n",
      "Epoch 1218: Loss = 0.960671\n",
      "Epoch 1219: Loss = 0.960602\n",
      "Epoch 1220: Loss = 0.960534\n",
      "Epoch 1221: Loss = 0.960467\n",
      "Epoch 1222: Loss = 0.960399\n",
      "Epoch 1223: Loss = 0.960330\n",
      "Epoch 1224: Loss = 0.960262\n",
      "Epoch 1225: Loss = 0.960194\n",
      "Epoch 1226: Loss = 0.960126\n",
      "Epoch 1227: Loss = 0.960058\n",
      "Epoch 1228: Loss = 0.959990\n",
      "Epoch 1229: Loss = 0.959922\n",
      "Epoch 1230: Loss = 0.959853\n",
      "Epoch 1231: Loss = 0.959782\n",
      "Epoch 1232: Loss = 0.959710\n",
      "Epoch 1233: Loss = 0.959640\n",
      "Epoch 1234: Loss = 0.959571\n",
      "Epoch 1235: Loss = 0.959503\n",
      "Epoch 1236: Loss = 0.959433\n",
      "Epoch 1237: Loss = 0.959364\n",
      "Epoch 1238: Loss = 0.959295\n",
      "Epoch 1239: Loss = 0.959225\n",
      "Epoch 1240: Loss = 0.959155\n",
      "Epoch 1241: Loss = 0.959085\n",
      "Epoch 1242: Loss = 0.959014\n",
      "Epoch 1243: Loss = 0.958942\n",
      "Epoch 1244: Loss = 0.958872\n",
      "Epoch 1245: Loss = 0.958802\n",
      "Epoch 1246: Loss = 0.958733\n",
      "Epoch 1247: Loss = 0.958664\n",
      "Epoch 1248: Loss = 0.958593\n",
      "Epoch 1249: Loss = 0.958522\n",
      "Epoch 1250: Loss = 0.958452\n",
      "Epoch 1251: Loss = 0.958383\n",
      "Epoch 1252: Loss = 0.958314\n",
      "Epoch 1253: Loss = 0.958244\n",
      "Epoch 1254: Loss = 0.958174\n",
      "Epoch 1255: Loss = 0.958104\n",
      "Epoch 1256: Loss = 0.958032\n",
      "Epoch 1257: Loss = 0.957958\n",
      "Epoch 1258: Loss = 0.957884\n",
      "Epoch 1259: Loss = 0.957814\n",
      "Epoch 1260: Loss = 0.957743\n",
      "Epoch 1261: Loss = 0.957672\n",
      "Epoch 1262: Loss = 0.957600\n",
      "Epoch 1263: Loss = 0.957528\n",
      "Epoch 1264: Loss = 0.957457\n",
      "Epoch 1265: Loss = 0.957385\n",
      "Epoch 1266: Loss = 0.957313\n",
      "Epoch 1267: Loss = 0.957240\n",
      "Epoch 1268: Loss = 0.957166\n",
      "Epoch 1269: Loss = 0.957094\n",
      "Epoch 1270: Loss = 0.957020\n",
      "Epoch 1271: Loss = 0.956948\n",
      "Epoch 1272: Loss = 0.956877\n",
      "Epoch 1273: Loss = 0.956805\n",
      "Epoch 1274: Loss = 0.956733\n",
      "Epoch 1275: Loss = 0.956661\n",
      "Epoch 1276: Loss = 0.956589\n",
      "Epoch 1277: Loss = 0.956516\n",
      "Epoch 1278: Loss = 0.956443\n",
      "Epoch 1279: Loss = 0.956370\n",
      "Epoch 1280: Loss = 0.956299\n",
      "Epoch 1281: Loss = 0.956226\n",
      "Epoch 1282: Loss = 0.956150\n",
      "Epoch 1283: Loss = 0.956074\n",
      "Epoch 1284: Loss = 0.956000\n",
      "Epoch 1285: Loss = 0.955928\n",
      "Epoch 1286: Loss = 0.955855\n",
      "Epoch 1287: Loss = 0.955782\n",
      "Epoch 1288: Loss = 0.955708\n",
      "Epoch 1289: Loss = 0.955634\n",
      "Epoch 1290: Loss = 0.955560\n",
      "Epoch 1291: Loss = 0.955487\n",
      "Epoch 1292: Loss = 0.955412\n",
      "Epoch 1293: Loss = 0.955337\n",
      "Epoch 1294: Loss = 0.955264\n",
      "Epoch 1295: Loss = 0.955189\n",
      "Epoch 1296: Loss = 0.955114\n",
      "Epoch 1297: Loss = 0.955041\n",
      "Epoch 1298: Loss = 0.954967\n",
      "Epoch 1299: Loss = 0.954893\n",
      "Epoch 1300: Loss = 0.954818\n",
      "Epoch 1301: Loss = 0.954744\n",
      "Epoch 1302: Loss = 0.954670\n",
      "Epoch 1303: Loss = 0.954596\n",
      "Epoch 1304: Loss = 0.954522\n",
      "Epoch 1305: Loss = 0.954447\n",
      "Epoch 1306: Loss = 0.954372\n",
      "Epoch 1307: Loss = 0.954295\n",
      "Epoch 1308: Loss = 0.954215\n",
      "Epoch 1309: Loss = 0.954140\n",
      "Epoch 1310: Loss = 0.954065\n",
      "Epoch 1311: Loss = 0.953990\n",
      "Epoch 1312: Loss = 0.953914\n",
      "Epoch 1313: Loss = 0.953838\n",
      "Epoch 1314: Loss = 0.953762\n",
      "Epoch 1315: Loss = 0.953686\n",
      "Epoch 1316: Loss = 0.953611\n",
      "Epoch 1317: Loss = 0.953535\n",
      "Epoch 1318: Loss = 0.953457\n",
      "Epoch 1319: Loss = 0.953380\n",
      "Epoch 1320: Loss = 0.953303\n",
      "Epoch 1321: Loss = 0.953225\n",
      "Epoch 1322: Loss = 0.953150\n",
      "Epoch 1323: Loss = 0.953074\n",
      "Epoch 1324: Loss = 0.952999\n",
      "Epoch 1325: Loss = 0.952922\n",
      "Epoch 1326: Loss = 0.952846\n",
      "Epoch 1327: Loss = 0.952769\n",
      "Epoch 1328: Loss = 0.952691\n",
      "Epoch 1329: Loss = 0.952615\n",
      "Epoch 1330: Loss = 0.952539\n",
      "Epoch 1331: Loss = 0.952462\n",
      "Epoch 1332: Loss = 0.952385\n",
      "Epoch 1333: Loss = 0.952303\n",
      "Epoch 1334: Loss = 0.952224\n",
      "Epoch 1335: Loss = 0.952149\n",
      "Epoch 1336: Loss = 0.952072\n",
      "Epoch 1337: Loss = 0.951995\n",
      "Epoch 1338: Loss = 0.951919\n",
      "Epoch 1339: Loss = 0.951840\n",
      "Epoch 1340: Loss = 0.951761\n",
      "Epoch 1341: Loss = 0.951682\n",
      "Epoch 1342: Loss = 0.951605\n",
      "Epoch 1343: Loss = 0.951526\n",
      "Epoch 1344: Loss = 0.951445\n",
      "Epoch 1345: Loss = 0.951368\n",
      "Epoch 1346: Loss = 0.951289\n",
      "Epoch 1347: Loss = 0.951212\n",
      "Epoch 1348: Loss = 0.951135\n",
      "Epoch 1349: Loss = 0.951057\n",
      "Epoch 1350: Loss = 0.950978\n",
      "Epoch 1351: Loss = 0.950900\n",
      "Epoch 1352: Loss = 0.950821\n",
      "Epoch 1353: Loss = 0.950743\n",
      "Epoch 1354: Loss = 0.950664\n",
      "Epoch 1355: Loss = 0.950586\n",
      "Epoch 1356: Loss = 0.950506\n",
      "Epoch 1357: Loss = 0.950425\n",
      "Epoch 1358: Loss = 0.950341\n",
      "Epoch 1359: Loss = 0.950260\n",
      "Epoch 1360: Loss = 0.950182\n",
      "Epoch 1361: Loss = 0.950103\n",
      "Epoch 1362: Loss = 0.950023\n",
      "Epoch 1363: Loss = 0.949944\n",
      "Epoch 1364: Loss = 0.949863\n",
      "Epoch 1365: Loss = 0.949783\n",
      "Epoch 1366: Loss = 0.949703\n",
      "Epoch 1367: Loss = 0.949624\n",
      "Epoch 1368: Loss = 0.949543\n",
      "Epoch 1369: Loss = 0.949461\n",
      "Epoch 1370: Loss = 0.949379\n",
      "Epoch 1371: Loss = 0.949297\n",
      "Epoch 1372: Loss = 0.949216\n",
      "Epoch 1373: Loss = 0.949137\n",
      "Epoch 1374: Loss = 0.949057\n",
      "Epoch 1375: Loss = 0.948978\n",
      "Epoch 1376: Loss = 0.948899\n",
      "Epoch 1377: Loss = 0.948820\n",
      "Epoch 1378: Loss = 0.948739\n",
      "Epoch 1379: Loss = 0.948658\n",
      "Epoch 1380: Loss = 0.948576\n",
      "Epoch 1381: Loss = 0.948495\n",
      "Epoch 1382: Loss = 0.948414\n",
      "Epoch 1383: Loss = 0.948328\n",
      "Epoch 1384: Loss = 0.948244\n",
      "Epoch 1385: Loss = 0.948163\n",
      "Epoch 1386: Loss = 0.948081\n",
      "Epoch 1387: Loss = 0.948000\n",
      "Epoch 1388: Loss = 0.947918\n",
      "Epoch 1389: Loss = 0.947836\n",
      "Epoch 1390: Loss = 0.947753\n",
      "Epoch 1391: Loss = 0.947673\n",
      "Epoch 1392: Loss = 0.947590\n",
      "Epoch 1393: Loss = 0.947506\n",
      "Epoch 1394: Loss = 0.947422\n",
      "Epoch 1395: Loss = 0.947340\n",
      "Epoch 1396: Loss = 0.947257\n",
      "Epoch 1397: Loss = 0.947175\n",
      "Epoch 1398: Loss = 0.947093\n",
      "Epoch 1399: Loss = 0.947011\n",
      "Epoch 1400: Loss = 0.946928\n",
      "Epoch 1401: Loss = 0.946845\n",
      "Epoch 1402: Loss = 0.946763\n",
      "Epoch 1403: Loss = 0.946680\n",
      "Epoch 1404: Loss = 0.946599\n",
      "Epoch 1405: Loss = 0.946515\n",
      "Epoch 1406: Loss = 0.946431\n",
      "Epoch 1407: Loss = 0.946348\n",
      "Epoch 1408: Loss = 0.946261\n",
      "Epoch 1409: Loss = 0.946174\n",
      "Epoch 1410: Loss = 0.946091\n",
      "Epoch 1411: Loss = 0.946007\n",
      "Epoch 1412: Loss = 0.945924\n",
      "Epoch 1413: Loss = 0.945841\n",
      "Epoch 1414: Loss = 0.945756\n",
      "Epoch 1415: Loss = 0.945670\n",
      "Epoch 1416: Loss = 0.945586\n",
      "Epoch 1417: Loss = 0.945503\n",
      "Epoch 1418: Loss = 0.945417\n",
      "Epoch 1419: Loss = 0.945333\n",
      "Epoch 1420: Loss = 0.945248\n",
      "Epoch 1421: Loss = 0.945161\n",
      "Epoch 1422: Loss = 0.945076\n",
      "Epoch 1423: Loss = 0.944991\n",
      "Epoch 1424: Loss = 0.944908\n",
      "Epoch 1425: Loss = 0.944824\n",
      "Epoch 1426: Loss = 0.944740\n",
      "Epoch 1427: Loss = 0.944654\n",
      "Epoch 1428: Loss = 0.944570\n",
      "Epoch 1429: Loss = 0.944484\n",
      "Epoch 1430: Loss = 0.944399\n",
      "Epoch 1431: Loss = 0.944313\n",
      "Epoch 1432: Loss = 0.944227\n",
      "Epoch 1433: Loss = 0.944139\n",
      "Epoch 1434: Loss = 0.944049\n",
      "Epoch 1435: Loss = 0.943963\n",
      "Epoch 1436: Loss = 0.943877\n",
      "Epoch 1437: Loss = 0.943791\n",
      "Epoch 1438: Loss = 0.943707\n",
      "Epoch 1439: Loss = 0.943622\n",
      "Epoch 1440: Loss = 0.943535\n",
      "Epoch 1441: Loss = 0.943448\n",
      "Epoch 1442: Loss = 0.943362\n",
      "Epoch 1443: Loss = 0.943276\n",
      "Epoch 1444: Loss = 0.943186\n",
      "Epoch 1445: Loss = 0.943101\n",
      "Epoch 1446: Loss = 0.943011\n",
      "Epoch 1447: Loss = 0.942924\n",
      "Epoch 1448: Loss = 0.942837\n",
      "Epoch 1449: Loss = 0.942750\n",
      "Epoch 1450: Loss = 0.942663\n",
      "Epoch 1451: Loss = 0.942577\n",
      "Epoch 1452: Loss = 0.942491\n",
      "Epoch 1453: Loss = 0.942402\n",
      "Epoch 1454: Loss = 0.942315\n",
      "Epoch 1455: Loss = 0.942230\n",
      "Epoch 1456: Loss = 0.942141\n",
      "Epoch 1457: Loss = 0.942052\n",
      "Epoch 1458: Loss = 0.941961\n",
      "Epoch 1459: Loss = 0.941869\n",
      "Epoch 1460: Loss = 0.941781\n",
      "Epoch 1461: Loss = 0.941693\n",
      "Epoch 1462: Loss = 0.941605\n",
      "Epoch 1463: Loss = 0.941518\n",
      "Epoch 1464: Loss = 0.941431\n",
      "Epoch 1465: Loss = 0.941342\n",
      "Epoch 1466: Loss = 0.941251\n",
      "Epoch 1467: Loss = 0.941163\n",
      "Epoch 1468: Loss = 0.941076\n",
      "Epoch 1469: Loss = 0.940985\n",
      "Epoch 1470: Loss = 0.940896\n",
      "Epoch 1471: Loss = 0.940806\n",
      "Epoch 1472: Loss = 0.940715\n",
      "Epoch 1473: Loss = 0.940626\n",
      "Epoch 1474: Loss = 0.940537\n",
      "Epoch 1475: Loss = 0.940447\n",
      "Epoch 1476: Loss = 0.940359\n",
      "Epoch 1477: Loss = 0.940272\n",
      "Epoch 1478: Loss = 0.940183\n",
      "Epoch 1479: Loss = 0.940094\n",
      "Epoch 1480: Loss = 0.940003\n",
      "Epoch 1481: Loss = 0.939911\n",
      "Epoch 1482: Loss = 0.939822\n",
      "Epoch 1483: Loss = 0.939731\n",
      "Epoch 1484: Loss = 0.939636\n",
      "Epoch 1485: Loss = 0.939542\n",
      "Epoch 1486: Loss = 0.939451\n",
      "Epoch 1487: Loss = 0.939359\n",
      "Epoch 1488: Loss = 0.939271\n",
      "Epoch 1489: Loss = 0.939182\n",
      "Epoch 1490: Loss = 0.939091\n",
      "Epoch 1491: Loss = 0.939001\n",
      "Epoch 1492: Loss = 0.938911\n",
      "Epoch 1493: Loss = 0.938820\n",
      "Epoch 1494: Loss = 0.938728\n",
      "Epoch 1495: Loss = 0.938637\n",
      "Epoch 1496: Loss = 0.938545\n",
      "Epoch 1497: Loss = 0.938451\n",
      "Epoch 1498: Loss = 0.938361\n",
      "Epoch 1499: Loss = 0.938270\n",
      "Epoch 1500: Loss = 0.938178\n",
      "Epoch 1501: Loss = 0.938087\n",
      "Epoch 1502: Loss = 0.937997\n",
      "Epoch 1503: Loss = 0.937905\n",
      "Epoch 1504: Loss = 0.937812\n",
      "Epoch 1505: Loss = 0.937722\n",
      "Epoch 1506: Loss = 0.937631\n",
      "Epoch 1507: Loss = 0.937537\n",
      "Epoch 1508: Loss = 0.937444\n",
      "Epoch 1509: Loss = 0.937348\n",
      "Epoch 1510: Loss = 0.937251\n",
      "Epoch 1511: Loss = 0.937158\n",
      "Epoch 1512: Loss = 0.937065\n",
      "Epoch 1513: Loss = 0.936971\n",
      "Epoch 1514: Loss = 0.936878\n",
      "Epoch 1515: Loss = 0.936786\n",
      "Epoch 1516: Loss = 0.936691\n",
      "Epoch 1517: Loss = 0.936598\n",
      "Epoch 1518: Loss = 0.936505\n",
      "Epoch 1519: Loss = 0.936412\n",
      "Epoch 1520: Loss = 0.936318\n",
      "Epoch 1521: Loss = 0.936224\n",
      "Epoch 1522: Loss = 0.936129\n",
      "Epoch 1523: Loss = 0.936034\n",
      "Epoch 1524: Loss = 0.935941\n",
      "Epoch 1525: Loss = 0.935847\n",
      "Epoch 1526: Loss = 0.935754\n",
      "Epoch 1527: Loss = 0.935660\n",
      "Epoch 1528: Loss = 0.935567\n",
      "Epoch 1529: Loss = 0.935473\n",
      "Epoch 1530: Loss = 0.935378\n",
      "Epoch 1531: Loss = 0.935285\n",
      "Epoch 1532: Loss = 0.935190\n",
      "Epoch 1533: Loss = 0.935094\n",
      "Epoch 1534: Loss = 0.934996\n",
      "Epoch 1535: Loss = 0.934900\n",
      "Epoch 1536: Loss = 0.934804\n",
      "Epoch 1537: Loss = 0.934707\n",
      "Epoch 1538: Loss = 0.934612\n",
      "Epoch 1539: Loss = 0.934519\n",
      "Epoch 1540: Loss = 0.934425\n",
      "Epoch 1541: Loss = 0.934327\n",
      "Epoch 1542: Loss = 0.934232\n",
      "Epoch 1543: Loss = 0.934137\n",
      "Epoch 1544: Loss = 0.934042\n",
      "Epoch 1545: Loss = 0.933947\n",
      "Epoch 1546: Loss = 0.933851\n",
      "Epoch 1547: Loss = 0.933753\n",
      "Epoch 1548: Loss = 0.933656\n",
      "Epoch 1549: Loss = 0.933562\n",
      "Epoch 1550: Loss = 0.933467\n",
      "Epoch 1551: Loss = 0.933371\n",
      "Epoch 1552: Loss = 0.933275\n",
      "Epoch 1553: Loss = 0.933179\n",
      "Epoch 1554: Loss = 0.933082\n",
      "Epoch 1555: Loss = 0.932986\n",
      "Epoch 1556: Loss = 0.932889\n",
      "Epoch 1557: Loss = 0.932792\n",
      "Epoch 1558: Loss = 0.932694\n",
      "Epoch 1559: Loss = 0.932596\n",
      "Epoch 1560: Loss = 0.932494\n",
      "Epoch 1561: Loss = 0.932392\n",
      "Epoch 1562: Loss = 0.932295\n",
      "Epoch 1563: Loss = 0.932195\n",
      "Epoch 1564: Loss = 0.932098\n",
      "Epoch 1565: Loss = 0.932003\n",
      "Epoch 1566: Loss = 0.931901\n",
      "Epoch 1567: Loss = 0.931804\n",
      "Epoch 1568: Loss = 0.931707\n",
      "Epoch 1569: Loss = 0.931609\n",
      "Epoch 1570: Loss = 0.931510\n",
      "Epoch 1571: Loss = 0.931413\n",
      "Epoch 1572: Loss = 0.931313\n",
      "Epoch 1573: Loss = 0.931213\n",
      "Epoch 1574: Loss = 0.931114\n",
      "Epoch 1575: Loss = 0.931017\n",
      "Epoch 1576: Loss = 0.930920\n",
      "Epoch 1577: Loss = 0.930823\n",
      "Epoch 1578: Loss = 0.930725\n",
      "Epoch 1579: Loss = 0.930629\n",
      "Epoch 1580: Loss = 0.930529\n",
      "Epoch 1581: Loss = 0.930429\n",
      "Epoch 1582: Loss = 0.930331\n",
      "Epoch 1583: Loss = 0.930231\n",
      "Epoch 1584: Loss = 0.930130\n",
      "Epoch 1585: Loss = 0.930027\n",
      "Epoch 1586: Loss = 0.929924\n",
      "Epoch 1587: Loss = 0.929824\n",
      "Epoch 1588: Loss = 0.929724\n",
      "Epoch 1589: Loss = 0.929622\n",
      "Epoch 1590: Loss = 0.929525\n",
      "Epoch 1591: Loss = 0.929425\n",
      "Epoch 1592: Loss = 0.929326\n",
      "Epoch 1593: Loss = 0.929226\n",
      "Epoch 1594: Loss = 0.929128\n",
      "Epoch 1595: Loss = 0.929027\n",
      "Epoch 1596: Loss = 0.928928\n",
      "Epoch 1597: Loss = 0.928827\n",
      "Epoch 1598: Loss = 0.928725\n",
      "Epoch 1599: Loss = 0.928624\n",
      "Epoch 1600: Loss = 0.928525\n",
      "Epoch 1601: Loss = 0.928426\n",
      "Epoch 1602: Loss = 0.928324\n",
      "Epoch 1603: Loss = 0.928224\n",
      "Epoch 1604: Loss = 0.928124\n",
      "Epoch 1605: Loss = 0.928022\n",
      "Epoch 1606: Loss = 0.927919\n",
      "Epoch 1607: Loss = 0.927818\n",
      "Epoch 1608: Loss = 0.927716\n",
      "Epoch 1609: Loss = 0.927612\n",
      "Epoch 1610: Loss = 0.927509\n",
      "Epoch 1611: Loss = 0.927404\n",
      "Epoch 1612: Loss = 0.927298\n",
      "Epoch 1613: Loss = 0.927194\n",
      "Epoch 1614: Loss = 0.927090\n",
      "Epoch 1615: Loss = 0.926987\n",
      "Epoch 1616: Loss = 0.926885\n",
      "Epoch 1617: Loss = 0.926782\n",
      "Epoch 1618: Loss = 0.926680\n",
      "Epoch 1619: Loss = 0.926579\n",
      "Epoch 1620: Loss = 0.926476\n",
      "Epoch 1621: Loss = 0.926373\n",
      "Epoch 1622: Loss = 0.926271\n",
      "Epoch 1623: Loss = 0.926170\n",
      "Epoch 1624: Loss = 0.926064\n",
      "Epoch 1625: Loss = 0.925961\n",
      "Epoch 1626: Loss = 0.925861\n",
      "Epoch 1627: Loss = 0.925756\n",
      "Epoch 1628: Loss = 0.925652\n",
      "Epoch 1629: Loss = 0.925549\n",
      "Epoch 1630: Loss = 0.925447\n",
      "Epoch 1631: Loss = 0.925342\n",
      "Epoch 1632: Loss = 0.925238\n",
      "Epoch 1633: Loss = 0.925135\n",
      "Epoch 1634: Loss = 0.925029\n",
      "Epoch 1635: Loss = 0.924924\n",
      "Epoch 1636: Loss = 0.924819\n",
      "Epoch 1637: Loss = 0.924713\n",
      "Epoch 1638: Loss = 0.924606\n",
      "Epoch 1639: Loss = 0.924502\n",
      "Epoch 1640: Loss = 0.924397\n",
      "Epoch 1641: Loss = 0.924293\n",
      "Epoch 1642: Loss = 0.924190\n",
      "Epoch 1643: Loss = 0.924085\n",
      "Epoch 1644: Loss = 0.923980\n",
      "Epoch 1645: Loss = 0.923877\n",
      "Epoch 1646: Loss = 0.923774\n",
      "Epoch 1647: Loss = 0.923670\n",
      "Epoch 1648: Loss = 0.923564\n",
      "Epoch 1649: Loss = 0.923458\n",
      "Epoch 1650: Loss = 0.923353\n",
      "Epoch 1651: Loss = 0.923249\n",
      "Epoch 1652: Loss = 0.923145\n",
      "Epoch 1653: Loss = 0.923038\n",
      "Epoch 1654: Loss = 0.922934\n",
      "Epoch 1655: Loss = 0.922829\n",
      "Epoch 1656: Loss = 0.922723\n",
      "Epoch 1657: Loss = 0.922614\n",
      "Epoch 1658: Loss = 0.922507\n",
      "Epoch 1659: Loss = 0.922400\n",
      "Epoch 1660: Loss = 0.922293\n",
      "Epoch 1661: Loss = 0.922185\n",
      "Epoch 1662: Loss = 0.922076\n",
      "Epoch 1663: Loss = 0.921965\n",
      "Epoch 1664: Loss = 0.921856\n",
      "Epoch 1665: Loss = 0.921747\n",
      "Epoch 1666: Loss = 0.921639\n",
      "Epoch 1667: Loss = 0.921531\n",
      "Epoch 1668: Loss = 0.921424\n",
      "Epoch 1669: Loss = 0.921316\n",
      "Epoch 1670: Loss = 0.921207\n",
      "Epoch 1671: Loss = 0.921099\n",
      "Epoch 1672: Loss = 0.920994\n",
      "Epoch 1673: Loss = 0.920885\n",
      "Epoch 1674: Loss = 0.920778\n",
      "Epoch 1675: Loss = 0.920670\n",
      "Epoch 1676: Loss = 0.920562\n",
      "Epoch 1677: Loss = 0.920455\n",
      "Epoch 1678: Loss = 0.920346\n",
      "Epoch 1679: Loss = 0.920236\n",
      "Epoch 1680: Loss = 0.920131\n",
      "Epoch 1681: Loss = 0.920023\n",
      "Epoch 1682: Loss = 0.919914\n",
      "Epoch 1683: Loss = 0.919803\n",
      "Epoch 1684: Loss = 0.919696\n",
      "Epoch 1685: Loss = 0.919586\n",
      "Epoch 1686: Loss = 0.919476\n",
      "Epoch 1687: Loss = 0.919365\n",
      "Epoch 1688: Loss = 0.919257\n",
      "Epoch 1689: Loss = 0.919143\n",
      "Epoch 1690: Loss = 0.919035\n",
      "Epoch 1691: Loss = 0.918926\n",
      "Epoch 1692: Loss = 0.918818\n",
      "Epoch 1693: Loss = 0.918708\n",
      "Epoch 1694: Loss = 0.918598\n",
      "Epoch 1695: Loss = 0.918490\n",
      "Epoch 1696: Loss = 0.918381\n",
      "Epoch 1697: Loss = 0.918274\n",
      "Epoch 1698: Loss = 0.918164\n",
      "Epoch 1699: Loss = 0.918054\n",
      "Epoch 1700: Loss = 0.917944\n",
      "Epoch 1701: Loss = 0.917834\n",
      "Epoch 1702: Loss = 0.917728\n",
      "Epoch 1703: Loss = 0.917619\n",
      "Epoch 1704: Loss = 0.917509\n",
      "Epoch 1705: Loss = 0.917400\n",
      "Epoch 1706: Loss = 0.917290\n",
      "Epoch 1707: Loss = 0.917179\n",
      "Epoch 1708: Loss = 0.917066\n",
      "Epoch 1709: Loss = 0.916954\n",
      "Epoch 1710: Loss = 0.916842\n",
      "Epoch 1711: Loss = 0.916725\n",
      "Epoch 1712: Loss = 0.916613\n",
      "Epoch 1713: Loss = 0.916498\n",
      "Epoch 1714: Loss = 0.916385\n",
      "Epoch 1715: Loss = 0.916269\n",
      "Epoch 1716: Loss = 0.916157\n",
      "Epoch 1717: Loss = 0.916044\n",
      "Epoch 1718: Loss = 0.915932\n",
      "Epoch 1719: Loss = 0.915820\n",
      "Epoch 1720: Loss = 0.915707\n",
      "Epoch 1721: Loss = 0.915593\n",
      "Epoch 1722: Loss = 0.915480\n",
      "Epoch 1723: Loss = 0.915369\n",
      "Epoch 1724: Loss = 0.915253\n",
      "Epoch 1725: Loss = 0.915142\n",
      "Epoch 1726: Loss = 0.915030\n",
      "Epoch 1727: Loss = 0.914918\n",
      "Epoch 1728: Loss = 0.914806\n",
      "Epoch 1729: Loss = 0.914693\n",
      "Epoch 1730: Loss = 0.914579\n",
      "Epoch 1731: Loss = 0.914469\n",
      "Epoch 1732: Loss = 0.914355\n",
      "Epoch 1733: Loss = 0.914242\n",
      "Epoch 1734: Loss = 0.914127\n",
      "Epoch 1735: Loss = 0.914013\n",
      "Epoch 1736: Loss = 0.913901\n",
      "Epoch 1737: Loss = 0.913785\n",
      "Epoch 1738: Loss = 0.913670\n",
      "Epoch 1739: Loss = 0.913554\n",
      "Epoch 1740: Loss = 0.913438\n",
      "Epoch 1741: Loss = 0.913321\n",
      "Epoch 1742: Loss = 0.913209\n",
      "Epoch 1743: Loss = 0.913098\n",
      "Epoch 1744: Loss = 0.912985\n",
      "Epoch 1745: Loss = 0.912872\n",
      "Epoch 1746: Loss = 0.912759\n",
      "Epoch 1747: Loss = 0.912644\n",
      "Epoch 1748: Loss = 0.912532\n",
      "Epoch 1749: Loss = 0.912417\n",
      "Epoch 1750: Loss = 0.912302\n",
      "Epoch 1751: Loss = 0.912188\n",
      "Epoch 1752: Loss = 0.912073\n",
      "Epoch 1753: Loss = 0.911959\n",
      "Epoch 1754: Loss = 0.911844\n",
      "Epoch 1755: Loss = 0.911731\n",
      "Epoch 1756: Loss = 0.911615\n",
      "Epoch 1757: Loss = 0.911501\n",
      "Epoch 1758: Loss = 0.911386\n",
      "Epoch 1759: Loss = 0.911268\n",
      "Epoch 1760: Loss = 0.911150\n",
      "Epoch 1761: Loss = 0.911032\n",
      "Epoch 1762: Loss = 0.910912\n",
      "Epoch 1763: Loss = 0.910793\n",
      "Epoch 1764: Loss = 0.910675\n",
      "Epoch 1765: Loss = 0.910557\n",
      "Epoch 1766: Loss = 0.910436\n",
      "Epoch 1767: Loss = 0.910316\n",
      "Epoch 1768: Loss = 0.910199\n",
      "Epoch 1769: Loss = 0.910079\n",
      "Epoch 1770: Loss = 0.909962\n",
      "Epoch 1771: Loss = 0.909847\n",
      "Epoch 1772: Loss = 0.909729\n",
      "Epoch 1773: Loss = 0.909611\n",
      "Epoch 1774: Loss = 0.909494\n",
      "Epoch 1775: Loss = 0.909376\n",
      "Epoch 1776: Loss = 0.909259\n",
      "Epoch 1777: Loss = 0.909141\n",
      "Epoch 1778: Loss = 0.909024\n",
      "Epoch 1779: Loss = 0.908906\n",
      "Epoch 1780: Loss = 0.908787\n",
      "Epoch 1781: Loss = 0.908670\n",
      "Epoch 1782: Loss = 0.908554\n",
      "Epoch 1783: Loss = 0.908437\n",
      "Epoch 1784: Loss = 0.908319\n",
      "Epoch 1785: Loss = 0.908203\n",
      "Epoch 1786: Loss = 0.908083\n",
      "Epoch 1787: Loss = 0.907964\n",
      "Epoch 1788: Loss = 0.907845\n",
      "Epoch 1789: Loss = 0.907725\n",
      "Epoch 1790: Loss = 0.907605\n",
      "Epoch 1791: Loss = 0.907483\n",
      "Epoch 1792: Loss = 0.907361\n",
      "Epoch 1793: Loss = 0.907243\n",
      "Epoch 1794: Loss = 0.907126\n",
      "Epoch 1795: Loss = 0.907009\n",
      "Epoch 1796: Loss = 0.906891\n",
      "Epoch 1797: Loss = 0.906774\n",
      "Epoch 1798: Loss = 0.906654\n",
      "Epoch 1799: Loss = 0.906536\n",
      "Epoch 1800: Loss = 0.906417\n",
      "Epoch 1801: Loss = 0.906298\n",
      "Epoch 1802: Loss = 0.906179\n",
      "Epoch 1803: Loss = 0.906060\n",
      "Epoch 1804: Loss = 0.905941\n",
      "Epoch 1805: Loss = 0.905820\n",
      "Epoch 1806: Loss = 0.905700\n",
      "Epoch 1807: Loss = 0.905581\n",
      "Epoch 1808: Loss = 0.905462\n",
      "Epoch 1809: Loss = 0.905342\n",
      "Epoch 1810: Loss = 0.905220\n",
      "Epoch 1811: Loss = 0.905098\n",
      "Epoch 1812: Loss = 0.904974\n",
      "Epoch 1813: Loss = 0.904851\n",
      "Epoch 1814: Loss = 0.904727\n",
      "Epoch 1815: Loss = 0.904603\n",
      "Epoch 1816: Loss = 0.904478\n",
      "Epoch 1817: Loss = 0.904354\n",
      "Epoch 1818: Loss = 0.904228\n",
      "Epoch 1819: Loss = 0.904105\n",
      "Epoch 1820: Loss = 0.903982\n",
      "Epoch 1821: Loss = 0.903858\n",
      "Epoch 1822: Loss = 0.903739\n",
      "Epoch 1823: Loss = 0.903618\n",
      "Epoch 1824: Loss = 0.903494\n",
      "Epoch 1825: Loss = 0.903372\n",
      "Epoch 1826: Loss = 0.903248\n",
      "Epoch 1827: Loss = 0.903128\n",
      "Epoch 1828: Loss = 0.903009\n",
      "Epoch 1829: Loss = 0.902886\n",
      "Epoch 1830: Loss = 0.902764\n",
      "Epoch 1831: Loss = 0.902639\n",
      "Epoch 1832: Loss = 0.902518\n",
      "Epoch 1833: Loss = 0.902396\n",
      "Epoch 1834: Loss = 0.902272\n",
      "Epoch 1835: Loss = 0.902151\n",
      "Epoch 1836: Loss = 0.902029\n",
      "Epoch 1837: Loss = 0.901904\n",
      "Epoch 1838: Loss = 0.901781\n",
      "Epoch 1839: Loss = 0.901656\n",
      "Epoch 1840: Loss = 0.901529\n",
      "Epoch 1841: Loss = 0.901405\n",
      "Epoch 1842: Loss = 0.901278\n",
      "Epoch 1843: Loss = 0.901152\n",
      "Epoch 1844: Loss = 0.901028\n",
      "Epoch 1845: Loss = 0.900906\n",
      "Epoch 1846: Loss = 0.900783\n",
      "Epoch 1847: Loss = 0.900659\n",
      "Epoch 1848: Loss = 0.900538\n",
      "Epoch 1849: Loss = 0.900415\n",
      "Epoch 1850: Loss = 0.900292\n",
      "Epoch 1851: Loss = 0.900168\n",
      "Epoch 1852: Loss = 0.900045\n",
      "Epoch 1853: Loss = 0.899922\n",
      "Epoch 1854: Loss = 0.899798\n",
      "Epoch 1855: Loss = 0.899672\n",
      "Epoch 1856: Loss = 0.899545\n",
      "Epoch 1857: Loss = 0.899422\n",
      "Epoch 1858: Loss = 0.899299\n",
      "Epoch 1859: Loss = 0.899171\n",
      "Epoch 1860: Loss = 0.899048\n",
      "Epoch 1861: Loss = 0.898921\n",
      "Epoch 1862: Loss = 0.898795\n",
      "Epoch 1863: Loss = 0.898666\n",
      "Epoch 1864: Loss = 0.898538\n",
      "Epoch 1865: Loss = 0.898410\n",
      "Epoch 1866: Loss = 0.898282\n",
      "Epoch 1867: Loss = 0.898153\n",
      "Epoch 1868: Loss = 0.898021\n",
      "Epoch 1869: Loss = 0.897893\n",
      "Epoch 1870: Loss = 0.897763\n",
      "Epoch 1871: Loss = 0.897636\n",
      "Epoch 1872: Loss = 0.897508\n",
      "Epoch 1873: Loss = 0.897382\n",
      "Epoch 1874: Loss = 0.897257\n",
      "Epoch 1875: Loss = 0.897129\n",
      "Epoch 1876: Loss = 0.897000\n",
      "Epoch 1877: Loss = 0.896870\n",
      "Epoch 1878: Loss = 0.896743\n",
      "Epoch 1879: Loss = 0.896619\n",
      "Epoch 1880: Loss = 0.896492\n",
      "Epoch 1881: Loss = 0.896365\n",
      "Epoch 1882: Loss = 0.896235\n",
      "Epoch 1883: Loss = 0.896108\n",
      "Epoch 1884: Loss = 0.895980\n",
      "Epoch 1885: Loss = 0.895851\n",
      "Epoch 1886: Loss = 0.895724\n",
      "Epoch 1887: Loss = 0.895598\n",
      "Epoch 1888: Loss = 0.895469\n",
      "Epoch 1889: Loss = 0.895341\n",
      "Epoch 1890: Loss = 0.895211\n",
      "Epoch 1891: Loss = 0.895080\n",
      "Epoch 1892: Loss = 0.894950\n",
      "Epoch 1893: Loss = 0.894820\n",
      "Epoch 1894: Loss = 0.894686\n",
      "Epoch 1895: Loss = 0.894557\n",
      "Epoch 1896: Loss = 0.894429\n",
      "Epoch 1897: Loss = 0.894302\n",
      "Epoch 1898: Loss = 0.894174\n",
      "Epoch 1899: Loss = 0.894047\n",
      "Epoch 1900: Loss = 0.893921\n",
      "Epoch 1901: Loss = 0.893791\n",
      "Epoch 1902: Loss = 0.893664\n",
      "Epoch 1903: Loss = 0.893535\n",
      "Epoch 1904: Loss = 0.893406\n",
      "Epoch 1905: Loss = 0.893279\n",
      "Epoch 1906: Loss = 0.893148\n",
      "Epoch 1907: Loss = 0.893017\n",
      "Epoch 1908: Loss = 0.892886\n",
      "Epoch 1909: Loss = 0.892758\n",
      "Epoch 1910: Loss = 0.892627\n",
      "Epoch 1911: Loss = 0.892500\n",
      "Epoch 1912: Loss = 0.892367\n",
      "Epoch 1913: Loss = 0.892236\n",
      "Epoch 1914: Loss = 0.892102\n",
      "Epoch 1915: Loss = 0.891970\n",
      "Epoch 1916: Loss = 0.891837\n",
      "Epoch 1917: Loss = 0.891704\n",
      "Epoch 1918: Loss = 0.891571\n",
      "Epoch 1919: Loss = 0.891432\n",
      "Epoch 1920: Loss = 0.891298\n",
      "Epoch 1921: Loss = 0.891164\n",
      "Epoch 1922: Loss = 0.891030\n",
      "Epoch 1923: Loss = 0.890899\n",
      "Epoch 1924: Loss = 0.890767\n",
      "Epoch 1925: Loss = 0.890635\n",
      "Epoch 1926: Loss = 0.890503\n",
      "Epoch 1927: Loss = 0.890368\n",
      "Epoch 1928: Loss = 0.890235\n",
      "Epoch 1929: Loss = 0.890104\n",
      "Epoch 1930: Loss = 0.889974\n",
      "Epoch 1931: Loss = 0.889844\n",
      "Epoch 1932: Loss = 0.889710\n",
      "Epoch 1933: Loss = 0.889577\n",
      "Epoch 1934: Loss = 0.889445\n",
      "Epoch 1935: Loss = 0.889311\n",
      "Epoch 1936: Loss = 0.889175\n",
      "Epoch 1937: Loss = 0.889045\n",
      "Epoch 1938: Loss = 0.888912\n",
      "Epoch 1939: Loss = 0.888778\n",
      "Epoch 1940: Loss = 0.888645\n",
      "Epoch 1941: Loss = 0.888510\n",
      "Epoch 1942: Loss = 0.888375\n",
      "Epoch 1943: Loss = 0.888240\n",
      "Epoch 1944: Loss = 0.888105\n",
      "Epoch 1945: Loss = 0.887965\n",
      "Epoch 1946: Loss = 0.887831\n",
      "Epoch 1947: Loss = 0.887699\n",
      "Epoch 1948: Loss = 0.887565\n",
      "Epoch 1949: Loss = 0.887432\n",
      "Epoch 1950: Loss = 0.887302\n",
      "Epoch 1951: Loss = 0.887169\n",
      "Epoch 1952: Loss = 0.887039\n",
      "Epoch 1953: Loss = 0.886905\n",
      "Epoch 1954: Loss = 0.886771\n",
      "Epoch 1955: Loss = 0.886637\n",
      "Epoch 1956: Loss = 0.886505\n",
      "Epoch 1957: Loss = 0.886370\n",
      "Epoch 1958: Loss = 0.886234\n",
      "Epoch 1959: Loss = 0.886100\n",
      "Epoch 1960: Loss = 0.885967\n",
      "Epoch 1961: Loss = 0.885830\n",
      "Epoch 1962: Loss = 0.885694\n",
      "Epoch 1963: Loss = 0.885559\n",
      "Epoch 1964: Loss = 0.885421\n",
      "Epoch 1965: Loss = 0.885280\n",
      "Epoch 1966: Loss = 0.885142\n",
      "Epoch 1967: Loss = 0.885006\n",
      "Epoch 1968: Loss = 0.884868\n",
      "Epoch 1969: Loss = 0.884731\n",
      "Epoch 1970: Loss = 0.884590\n",
      "Epoch 1971: Loss = 0.884446\n",
      "Epoch 1972: Loss = 0.884307\n",
      "Epoch 1973: Loss = 0.884169\n",
      "Epoch 1974: Loss = 0.884032\n",
      "Epoch 1975: Loss = 0.883893\n",
      "Epoch 1976: Loss = 0.883758\n",
      "Epoch 1977: Loss = 0.883622\n",
      "Epoch 1978: Loss = 0.883482\n",
      "Epoch 1979: Loss = 0.883343\n",
      "Epoch 1980: Loss = 0.883206\n",
      "Epoch 1981: Loss = 0.883071\n",
      "Epoch 1982: Loss = 0.882936\n",
      "Epoch 1983: Loss = 0.882797\n",
      "Epoch 1984: Loss = 0.882659\n",
      "Epoch 1985: Loss = 0.882520\n",
      "Epoch 1986: Loss = 0.882383\n",
      "Epoch 1987: Loss = 0.882245\n",
      "Epoch 1988: Loss = 0.882107\n",
      "Epoch 1989: Loss = 0.881971\n",
      "Epoch 1990: Loss = 0.881833\n",
      "Epoch 1991: Loss = 0.881694\n",
      "Epoch 1992: Loss = 0.881554\n",
      "Epoch 1993: Loss = 0.881417\n",
      "Epoch 1994: Loss = 0.881278\n",
      "Epoch 1995: Loss = 0.881138\n",
      "Epoch 1996: Loss = 0.880994\n",
      "Epoch 1997: Loss = 0.880853\n",
      "Epoch 1998: Loss = 0.880715\n",
      "Epoch 1999: Loss = 0.880577\n",
      "Epoch 2000: Loss = 0.880439\n",
      "Epoch 2001: Loss = 0.880301\n",
      "Epoch 2002: Loss = 0.880166\n",
      "Epoch 2003: Loss = 0.880027\n",
      "Epoch 2004: Loss = 0.879887\n",
      "Epoch 2005: Loss = 0.879747\n",
      "Epoch 2006: Loss = 0.879609\n",
      "Epoch 2007: Loss = 0.879470\n",
      "Epoch 2008: Loss = 0.879331\n",
      "Epoch 2009: Loss = 0.879190\n",
      "Epoch 2010: Loss = 0.879049\n",
      "Epoch 2011: Loss = 0.878910\n",
      "Epoch 2012: Loss = 0.878769\n",
      "Epoch 2013: Loss = 0.878629\n",
      "Epoch 2014: Loss = 0.878488\n",
      "Epoch 2015: Loss = 0.878346\n",
      "Epoch 2016: Loss = 0.878202\n",
      "Epoch 2017: Loss = 0.878061\n",
      "Epoch 2018: Loss = 0.877919\n",
      "Epoch 2019: Loss = 0.877777\n",
      "Epoch 2020: Loss = 0.877633\n",
      "Epoch 2021: Loss = 0.877489\n",
      "Epoch 2022: Loss = 0.877340\n",
      "Epoch 2023: Loss = 0.877195\n",
      "Epoch 2024: Loss = 0.877052\n",
      "Epoch 2025: Loss = 0.876909\n",
      "Epoch 2026: Loss = 0.876765\n",
      "Epoch 2027: Loss = 0.876625\n",
      "Epoch 2028: Loss = 0.876482\n",
      "Epoch 2029: Loss = 0.876338\n",
      "Epoch 2030: Loss = 0.876193\n",
      "Epoch 2031: Loss = 0.876052\n",
      "Epoch 2032: Loss = 0.875911\n",
      "Epoch 2033: Loss = 0.875768\n",
      "Epoch 2034: Loss = 0.875629\n",
      "Epoch 2035: Loss = 0.875485\n",
      "Epoch 2036: Loss = 0.875346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:29:41.608158: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2037: Loss = 0.875203\n",
      "Epoch 2038: Loss = 0.875062\n",
      "Epoch 2039: Loss = 0.874918\n",
      "Epoch 2040: Loss = 0.874777\n",
      "Epoch 2041: Loss = 0.874633\n",
      "Epoch 2042: Loss = 0.874488\n",
      "Epoch 2043: Loss = 0.874345\n",
      "Epoch 2044: Loss = 0.874202\n",
      "Epoch 2045: Loss = 0.874056\n",
      "Epoch 2046: Loss = 0.873909\n",
      "Epoch 2047: Loss = 0.873764\n",
      "Epoch 2048: Loss = 0.873615\n",
      "Epoch 2049: Loss = 0.873474\n",
      "Epoch 2050: Loss = 0.873330\n",
      "Epoch 2051: Loss = 0.873186\n",
      "Epoch 2052: Loss = 0.873041\n",
      "Epoch 2053: Loss = 0.872899\n",
      "Epoch 2054: Loss = 0.872756\n",
      "Epoch 2055: Loss = 0.872613\n",
      "Epoch 2056: Loss = 0.872469\n",
      "Epoch 2057: Loss = 0.872327\n",
      "Epoch 2058: Loss = 0.872181\n",
      "Epoch 2059: Loss = 0.872036\n",
      "Epoch 2060: Loss = 0.871890\n",
      "Epoch 2061: Loss = 0.871745\n",
      "Epoch 2062: Loss = 0.871602\n",
      "Epoch 2063: Loss = 0.871458\n",
      "Epoch 2064: Loss = 0.871312\n",
      "Epoch 2065: Loss = 0.871163\n",
      "Epoch 2066: Loss = 0.871017\n",
      "Epoch 2067: Loss = 0.870866\n",
      "Epoch 2068: Loss = 0.870720\n",
      "Epoch 2069: Loss = 0.870574\n",
      "Epoch 2070: Loss = 0.870429\n",
      "Epoch 2071: Loss = 0.870278\n",
      "Epoch 2072: Loss = 0.870129\n",
      "Epoch 2073: Loss = 0.869975\n",
      "Epoch 2074: Loss = 0.869825\n",
      "Epoch 2075: Loss = 0.869680\n",
      "Epoch 2076: Loss = 0.869531\n",
      "Epoch 2077: Loss = 0.869386\n",
      "Epoch 2078: Loss = 0.869238\n",
      "Epoch 2079: Loss = 0.869090\n",
      "Epoch 2080: Loss = 0.868944\n",
      "Epoch 2081: Loss = 0.868792\n",
      "Epoch 2082: Loss = 0.868645\n",
      "Epoch 2083: Loss = 0.868501\n",
      "Epoch 2084: Loss = 0.868354\n",
      "Epoch 2085: Loss = 0.868209\n",
      "Epoch 2086: Loss = 0.868060\n",
      "Epoch 2087: Loss = 0.867910\n",
      "Epoch 2088: Loss = 0.867762\n",
      "Epoch 2089: Loss = 0.867619\n",
      "Epoch 2090: Loss = 0.867471\n",
      "Epoch 2091: Loss = 0.867321\n",
      "Epoch 2092: Loss = 0.867173\n",
      "Epoch 2093: Loss = 0.867025\n",
      "Epoch 2094: Loss = 0.866877\n",
      "Epoch 2095: Loss = 0.866730\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optionally enable mixed precision if using GPUs\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision enabled\")\n",
    "\n",
    "# --- Data Dimensions ---\n",
    "num_molecules = 5         # CH4, CO2, CO2bio, GWP, N2O\n",
    "num_time = 288            # 12 months * 24 years\n",
    "lat_full = 600            # Full kernelized latitude dimension\n",
    "lon_full = 1200           # Full kernelized longitude dimension\n",
    "\n",
    "# We'll work in a lower resolution for this baseline:\n",
    "low_res_h, low_res_w = 75, 150\n",
    "output_channels = 5       # one channel per molecule\n",
    "\n",
    "# --- Use preloaded tensor ---\n",
    "# Assume 'kernelized_tensor' is loaded and has shape (5, 2, 288, 600, 1200)\n",
    "data = kernelized_tensor\n",
    "\n",
    "# Select emissions only (data type index 0); resulting shape: (5, 288, 600, 1200)\n",
    "data_emissions = data[:, 0, :, :, :]\n",
    "\n",
    "# Rearrange dimensions so that time is first: (288, 5, 600, 1200)\n",
    "emissions_data = np.transpose(data_emissions, (1, 0, 2, 3))\n",
    "\n",
    "# Convert targets to float32 and reshape to (num_time, 5, 600, 1200)\n",
    "Y_full = emissions_data.astype(np.float32).reshape(num_time, num_molecules, lat_full, lon_full)\n",
    "\n",
    "# --- Downsample Targets ---\n",
    "# Downsample from (600, 1200) to (75, 150) using block averaging.\n",
    "Y_full = np.transpose(Y_full, (0, 2, 3, 1))  # Now shape: (num_time, 600, 1200, 5)\n",
    "Y_low = np.empty((num_time, low_res_h, low_res_w, output_channels), dtype=np.float32)\n",
    "for i in range(num_time):\n",
    "    # Average over each 8x8 block (600/75=8 and 1200/150=8)\n",
    "    Y_low[i] = Y_full[i].reshape(low_res_h, 8, low_res_w, 8, output_channels).mean(axis=(1, 3))\n",
    "\n",
    "# --- Log Transformation ---\n",
    "# Apply log1p to compress the wide dynamic range (assumes targets are nonnegative)\n",
    "Y_low_log = np.log1p(Y_low)\n",
    "\n",
    "# --- Prepare Time Features ---\n",
    "# Encode the month cyclically (this helps capture seasonal/periodic patterns).\n",
    "months = np.arange(num_time, dtype=np.float32)\n",
    "month_norm = months / (num_time - 1)\n",
    "X = np.stack([np.sin(2 * np.pi * month_norm), np.cos(2 * np.pi * month_norm)], axis=1)\n",
    "# X shape: (num_time, 2)\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "train_time = 48  # First 4 years\n",
    "test_time = 12   # Next 12 months\n",
    "X_train = X[:train_time]\n",
    "X_test = X[train_time:train_time + test_time]\n",
    "Y_train = Y_low_log[:train_time]  # shape: (48, 75, 150, 5)\n",
    "Y_test = Y_low_log[train_time:train_time + test_time]\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Test samples:\", X_test.shape[0])\n",
    "print(\"Target (log-transformed, low-res) shape per sample:\", Y_train.shape[1:])  # Should be (75,150,5)\n",
    "\n",
    "# --- Normalize Log-Transformed Targets ---\n",
    "Y_train_mean = np.mean(Y_train, dtype=np.float32)\n",
    "Y_train_std = np.std(Y_train, dtype=np.float32)\n",
    "print(\"Y_train_mean (log scale):\", Y_train_mean)\n",
    "print(\"Y_train_std (log scale):\", Y_train_std)\n",
    "Y_train_norm = (Y_train - Y_train_mean) / Y_train_std\n",
    "Y_test_norm = (Y_test - Y_train_mean) / Y_train_std\n",
    "\n",
    "# --- Custom Callback ---\n",
    "class ProgressCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = logs.get('loss')\n",
    "        print(f\"Epoch {epoch+1:03d}: Loss = {loss:.6f}\")\n",
    "progress_callback = ProgressCallback()\n",
    "\n",
    "# --- GPU Setup ---\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    # Build a simple MLP model (this is an MLP, not a DenseNet).\n",
    "    # MLP stands for Multi-Layer Perceptron: it consists of fully connected (dense) layers.\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(2,)),  # Input: 2 features (sin and cos)\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        # Output a vector that reshapes to (75, 150, 5)\n",
    "        layers.Dense(low_res_h * low_res_w * output_channels, activation='relu'),\n",
    "        layers.Reshape((low_res_h, low_res_w, output_channels))\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=1e-7, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    model.summary()\n",
    "\n",
    "# --- Train the Model ---\n",
    "history = model.fit(\n",
    "    X_train, Y_train_norm,\n",
    "    epochs=20000,\n",
    "    batch_size=1,\n",
    "    verbose=0,\n",
    "    callbacks=[progress_callback]\n",
    ")\n",
    "\n",
    "# --- Plot Training Loss History ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'], marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss (MSE)')\n",
    "plt.title('Training Loss Progress')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Evaluate the Model ---\n",
    "Y_pred_norm = model.predict(X_test)\n",
    "test_mse_norm = mean_squared_error(Y_test_norm.flatten(), Y_pred_norm.flatten())\n",
    "print(\"Test MSE (normalized):\", test_mse_norm)\n",
    "\n",
    "# Inverse transform predictions:\n",
    "Y_pred_log = Y_pred_norm * Y_train_std + Y_train_mean\n",
    "# Invert the log transform using expm1 to recover original (low-resolution) scale\n",
    "Y_pred = np.expm1(Y_pred_log)\n",
    "Y_test_inv = np.expm1(Y_test_norm * Y_train_std + Y_train_mean)\n",
    "\n",
    "# --- Compute Additional Metrics ---\n",
    "\n",
    "# Standard metrics:\n",
    "mse = mean_squared_error(Y_test_inv.flatten(), Y_pred.flatten())\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(Y_test_inv.flatten(), Y_pred.flatten())\n",
    "r2 = r2_score(Y_test_inv.flatten(), Y_pred.flatten())\n",
    "corr = np.corrcoef(Y_test_inv.flatten(), Y_pred.flatten())[0, 1]\n",
    "\n",
    "# Define a custom \"precision\" for regression:\n",
    "# Here we count the fraction of predictions whose relative error is below a threshold (e.g., 10%).\n",
    "def regression_precision(y_true, y_pred, threshold=0.1):\n",
    "    epsilon = 1e-8  # small constant to avoid division by zero\n",
    "    relative_error = np.abs(y_true - y_pred) / (np.abs(y_true) + epsilon)\n",
    "    return np.mean(relative_error < threshold)\n",
    "\n",
    "precision = regression_precision(Y_test_inv.flatten(), Y_pred.flatten(), threshold=0.1)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R^2:\", r2)\n",
    "print(\"Pearson correlation coefficient:\", corr)\n",
    "print(\"Regression Precision (within 10% error):\", precision)\n",
    "\n",
    "# --- Visualize a Sample Prediction ---\n",
    "sample_idx = 0  # first test sample\n",
    "molecule_idx = 0  # visualize molecule 0\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(Y_test_inv[sample_idx, :, :, molecule_idx], cmap='viridis')\n",
    "plt.title('Ground Truth (Molecule 0)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(Y_pred[sample_idx, :, :, molecule_idx], cmap='viridis')\n",
    "plt.title('Prediction (Molecule 0)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verifiers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
